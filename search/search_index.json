{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello, I'm Omar","text":"<p>I\u2019m a software engineer specializing in artificial intelligence and machine learning, with strong interests in interpretability, alignment, and agentic systems. I build AI-powered products, explore how digital and biological brains work, and help startups find product\u2013market fit. Open to long-term moonshots and selective consulting engagements.</p>"},{"location":"#what-i-work-on","title":"What I work on","text":"<ol> <li>Building production-ready AI/ML systems (Agents, RAG, Evals, etc.)</li> <li>Consulting and training in AI/ML for startups</li> <li>Creating technical content and developer docs</li> <li>Conducting domain-specific research</li> </ol> <p>Fun fact: I once worked as a music producer and YouTube content creator. In my free time you can find me making beats, learning Mandarin (\u4f60\u597d), or exploring new places around the world.</p>"},{"location":"#lets-connect","title":"Let's Connect","text":"<p>Reach out via X, LinkedIn, or omar@olivares.cl to discuss your next project or just to chat about tech.</p>"},{"location":"blog/","title":"Blog","text":"<p>This is my blog. I write about my experiences, thoughts, and ideas.</p>"},{"location":"blog/2025/01/01/things-you-should-be-reading-about-ai-in-2025/","title":"Things you should be reading about AI in 2025","text":"<p>This year, my focus is on exploring the intersection of AI, Machine Learning, and Neuroscience. I'll be diving deep into reading material and codebases to better understand both the natural and artificial cognition. The following lists outline the books, textbooks, and code that I'm currently learning from and probably you should too if you are interested in AI, more than just the hype and actually care about the future of the field.</p> <p>I compiled the most cited books and textbooks from top UK/US schools to see if I can get a sense of what I should be learning in a Master's program and meaningfully try to contribute to the field.</p> <p>It's pretty easy to get lost in the countless number of papers you have to read to catch up with the latest research/engineering in AI right now. I default to the basics instead and I'm reading textbooks over papers lately. If you really want to be up to date with the latest research, it should be something you personally care about, you might want to head to Emergent Mind and check out all the papers that people are sharing right now (on X, Reddit, HN, etc.) because the space is moving so fast that as of today, this text might be deprecated too. But I won't advise people to start reading papers, because it's a lot of work and you'll probably get lost in a sea of noise.</p>"},{"location":"blog/2025/01/01/things-you-should-be-reading-about-ai-in-2025/#books","title":"Books","text":"<p>I'd advice you to start with the following books, which are a great starting point to fill your appetite for the field, and get a sense of the big picture, and where all we're going.</p>"},{"location":"blog/2025/01/01/things-you-should-be-reading-about-ai-in-2025/#textbooks","title":"Textbooks","text":"<p>Once you have a grasp of the big picture, you might want to get a glimpse of the technical aspects. These books require minimal prerequisites and focus on concepts rather than heavy mathematics, and are also succinct, which makes them great for a quick read.</p> <p>For those with some programming experience and basic math background. These books are a great starting point to understand the technical aspects of AI, and build a solid intuition before heading to the advanced level.</p> <p>Once you get at least comfortable with the math of machine learning<sup>1</sup>, you might be interested in tackling these, which I found to be highly recommended by many, many Master's and PhD curricula. I also found that some AI/ML programs might benefit a lot from borrowing inspiration from classic neuroscience program textbooks, because we've seen fruitful results in the past from this cross-pollination. Without going into specifics, from interpretability to reinforcement learning, there's much to learn from neuroscience.</p> <p>One small thing to note is that instead of the classic Pattern Recognition and Machine Learning<sup>2</sup> mentioned everywhere, I prefer the newer version from the same author and his son Deep Learning<sup>3</sup>, which is more up-to-date and includes more recent research that better reflects the state of the art.</p>"},{"location":"blog/2025/01/01/things-you-should-be-reading-about-ai-in-2025/#code","title":"Code","text":"<p>Besides reading material, I also think it's important to get hands-on coding experience with codebases and frameworks that are currently being used in the industry. I've compiled a list of some of the most popular projects in the field that are mentioned in many job postings and are used by top AI labs. These are the ones you should be looking at:</p> <ul> <li>PyTorch by Meta</li> <li>XLA &amp; JAX by Google DeepMind</li> <li>Transformers<sup>4</sup> by Hugging Face</li> <li>MLX<sup>5</sup>, MLX examples &amp; axlearn by Apple</li> <li>tinygrad by tiny corp</li> <li>ggml &amp; llama.cpp by Georgi Gerganov</li> <li>Triton by OpenAI<sup>6</sup></li> <li>CUDA by Nvidia &amp; GPU Puzzles by Sasha Rush</li> </ul> <p>And also some educational implementations:</p> <ul> <li>nanoGPT, micrograd, &amp; llm.c by Andrej Karpathy</li> <li>x-transfomers by Phil Wang (aka lucidrains)</li> </ul> <p>If you're not sure where to start, I'd recommend starting with tinygrad, MLX and MLX examples, which are minimalistic and easy to understand, and you can also run them using your own hardware if you own a Mac.</p> <p>If you get a strong foundation with all the contents above, you might be in high demand in the industry. If you have any questions or suggestions for this post, feel free to reach out to me on X, or suggest edits on GitHub.</p> <p>Happy learning!</p> <ol> <li> <p>M. P. Deisenroth, A. A. Faisal, and C. S. Ong, Mathematics for machine learning. Cambridge University Press, 2020. Available: https://mml-book.github.io/book/mml-book.pdf \u21a9</p> </li> <li> <p>C. M. Bishop and N. M. Nasrabadi, Pattern recognition and machine learning, vol. 4. Springer, 2006.\u00a0\u21a9</p> </li> <li> <p>C. M. Bishop and H. Bishop, Deep learning: Foundations and concepts. Springer, 2024. Available: https://www.bishopbook.com \u21a9</p> </li> <li> <p>T. Wolf et al., \"Transformers: State-of-the-art natural language processing,\" in Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations, Online: Association for Computational Linguistics, 2020, pp. 38--45. Available: https://www.aclweb.org/anthology/2020.emnlp-demos.6 \u21a9</p> </li> <li> <p>A. Awni et al., \"MLX.\" 2023. Available: https://github.com/ml-explore/mlx \u21a9</p> </li> <li> <p>P. Tillet, H.-T. Kung, and D. Cox, \"Triton: An intermediate language and compiler for tiled neural network computations.\" pp. 10--19, 2019. Available: https://www.eecs.harvard.edu/\\textasciitilde htk/publication/2019-mapl-tillet-kung-cox.pdf \u21a9</p> </li> </ol>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/","title":"Introduction to Tinygrad","text":"<p>Understanding Tinygrad can be hard, even Karpathy is a bit confused about it. This guide is my first attempt to navigate Tinygrad by synthesizing insights from official documentation and various livestreams.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#why-tinygrad","title":"Why Tinygrad?","text":"<p>Tinygrad seems different from other mature frameworks such as PyTorch, JAX, and TensorFlow etc. But to me, what it seems appealing is that they want to democratize the petaflop as their core mission.</p> <p>This is what I like about it:</p> <ol> <li>Fully open source (even its hiring process, but that's another story)</li> <li>No dependencies</li> <li>Multiple backends</li> <li>And a very tiny codebase</li> </ol> <p>On the other hand, what I still don't like:</p> <ol> <li>Steep learning curve, and not noob friendly</li> <li>Hard to read code</li> <li>Not the best documentation, and some broken examples</li> <li>It's not stable yet, but soon will be</li> </ol> <p>But let's start coding.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#installation","title":"Installation","text":"<p>Tinygrad is designed to be hackable from the ground up. The recommended installation uses the editable <code>-e</code> flag from pip, allowing you to edit the source code and see changes right away.</p> <pre><code>git clone https://github.com/Tinygrad/Tinygrad.git\ncd Tinygrad\npython3 -m pip install -e .\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#the-tensor","title":"The Tensor","text":"<p>Let's start with something familiar, thanks to Pytorch-like API, creating and manipulating tensors:</p> <pre><code>from tinygrad import Tensor\n\nt = Tensor([1, 2, 3, 4])\nprint(t)\n\n# Basic operations work as expected\ndoubled = t * 2\nprint(doubled.tolist())  # [2, 4, 6, 8]\n\n# More complex operations\nresult = t + 3 + 4\nprint(result.tolist())   # [8, 10, 12, 14]\n</code></pre> <p>This looks familiar if you've used PyTorch or NumPy. But Tinygrad is doing something fundamentally different under the hood, let's explore what makes it special.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#the-devices","title":"The Devices","text":"<p>Tinygrad's first key concept is the Device, where Tensors are stored and compute is executed. There are multiple devices supported, including CPU, CUDA, METAL, and more. For example, on my Apple Silicon macbook, the default device is METAL:</p> <pre><code>from tinygrad import Device\n\nprint(Device.DEFAULT)  # On Apple Silicon Mac: \"METAL\"\n</code></pre> <p>Tinygrad auto-detects the best available device on your system and makes it the default. This could be METAL (Apple Silicon), CUDA (NVIDIA), CPU, or other supported backends. The device abstraction allows the same code to run across different hardware.</p> <p>You can override the default device:</p> <pre><code>Device.DEFAULT = \"CPU\"\nprint(Device.DEFAULT)  # \"CPU\"\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Here's where Tinygrad becomes interesting. Let's examine what we actually created:</p> <pre><code>from tinygrad import Tensor, dtypes\n\nt = Tensor([1, 2, 3, 4])\n\n# Examine the tensor properties\nassert t.device == Device.DEFAULT\nassert t.dtype == dtypes.int\nassert t.shape == (4,)\n\nprint(t)\n# &lt;Tensor &lt;UOp CPU (4,) int (&lt;Ops.COPY: 7&gt;, None)&gt; on CPU with grad None&gt;\n</code></pre> <p>This Tensor has not been computed yet. Tinygrad is lazy - it builds a computational specification rather than immediately executing operations. The Tensor contains a chain of UOPs (micro-operations) that specify how to compute it when needed.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#the-uop","title":"The UOP","text":"<p>UOPs (micro-operations) are the core specification language in Tinygrad. They are immutable and form a DAG (Directed Acyclic Graph). Each UOP has an operation type, dtype, arguments, and source dependencies.</p> <pre><code>print(t.uop)\n</code></pre> <p>You'll see something like:</p> <pre><code>UOp(Ops.COPY, dtypes.int, arg=None, src=(\n  UOp(Ops.BUFFER, dtypes.int, arg=4, src=(\n    UOp(Ops.UNIQUE, dtypes.void, arg=0, src=()),\n    UOp(Ops.DEVICE, dtypes.void, arg='PYTHON', src=()),)),\n  UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),))\n</code></pre> <p>This UOP tree specifies a COPY operation from a BUFFER on the PYTHON device (where our Python list <code>[1,2,3,4]</code> lives) to the CPU device. The UNIQUE identifier ensures unambiguous reference to this specific buffer.</p> <pre><code>flowchart LR\n    A[\"COPY&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    B[\"BUFFER&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n    C[\"UNIQUE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=0\"]\n    D[\"DEVICE&lt;br/&gt;dtypes.void&lt;br/&gt;arg='PYTHON'\"]\n    E[\"DEVICE&lt;br/&gt;dtypes.void&lt;br/&gt;arg='CPU'\"]\n\n    A --&gt; B\n    A --&gt; E\n    B --&gt; C\n    B --&gt; D</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#realization","title":"Realization","text":"<p>To execute the computational specification, we use the <code>realize()</code> method:</p> <pre><code>t.realize()\nprint(t.uop)\n</code></pre> <p>After realization, the UOP changes fundamentally:</p> <pre><code>UOp(Ops.BUFFER, dtypes.int, arg=4, src=(\n  UOp(Ops.UNIQUE, dtypes.void, arg=1, src=()),\n  UOp(Ops.DEVICE, dtypes.void, arg='CPU', src=()),))\n</code></pre> <pre><code>flowchart LR\n    A[\"BUFFER&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n    B[\"UNIQUE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=1\"]\n    C[\"DEVICE&lt;br/&gt;dtypes.void&lt;br/&gt;arg='CPU'\"]\n\n    A --&gt; B\n    A --&gt; C</code></pre> <p>The COPY operation has been replaced by a simple BUFFER reference. The UNIQUE identifier changed from <code>arg=0</code> to <code>arg=1</code>, indicating the data now exists as a new computational artifact on the target device.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#operations-build-computation-graphs","title":"Operations build computation graphs","text":"<p>Let's see how operations create more complex graphs:</p> <pre><code>t_times_2 = t * 2\nprint(t_times_2.uop)\n</code></pre> <p>This creates a UOP tree with a MUL operation. What appears as simple scalar multiplication becomes an explicit broadcasting specification:</p> <pre><code>flowchart LR\n    A[\"MUL&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    B[\"BUFFER&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n    C[\"UNIQUE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=1\"]\n    D[\"DEVICE (x2)&lt;br/&gt;dtypes.void&lt;br/&gt;arg='CPU'\"]\n    E[\"EXPAND&lt;br/&gt;dtypes.int&lt;br/&gt;arg=(4,)\"]\n    F[\"RESHAPE&lt;br/&gt;dtypes.int&lt;br/&gt;arg=(1,)\"]\n    G[\"CONST&lt;br/&gt;dtypes.int&lt;br/&gt;arg=2\"]\n    H[\"VIEW&lt;br/&gt;dtypes.void&lt;br/&gt;arg=ShapeTracker\"]\n\n    A --&gt; B\n    A --&gt; E\n    B --&gt; C\n    B --&gt; D\n    E --&gt; F\n    F --&gt; G\n    G --&gt; H\n    H --&gt; D</code></pre> <p>Understanding Broadcasting: The scalar <code>2</code> is transformed through RESHAPE and EXPAND operations to match our Tensor's shape. This makes the computational intent explicit and optimizable.</p> <p>We can verify the result:</p> <pre><code>assert t_times_2.tolist() == [2, 4, 6, 8]\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#smart-deduplication","title":"Smart Deduplication","text":"<p>UOPs are both immutable and globally unique, leading to elegant computational deduplication:</p> <pre><code>A = t * 4\nB = t * 4\n\n# Different Python objects\nassert A is not B\n\n# Same computational specification\nassert A.uop is B.uop\n</code></pre> <p>When we realize one Tensor, both benefit from the shared computation:</p> <pre><code>A.realize()\n\n# Both tensors now point to the same realized buffer\nassert A.uop is B.uop\n\nprint(B.tolist())  # [4, 8, 12, 16] - no computation needed\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#automatic-differentiation","title":"Automatic Differentiation","text":"<p>Tinygrad includes automatic differentiation that operates on the same UOP principles. Let's see gradients in action:</p> <pre><code>x = Tensor([2.0], requires_grad=True)\ny = x**2 + 3*x + 1\nloss = y.sum()\n\nprint(x.item()) # 2.0\nprint(y.item()) # 4 + 6 + 1 = 11\n</code></pre> <p>For the function \\(y = x^2 + 3x + 1\\), the derivative is:</p> \\[\\frac{dy}{dx} = 2x + 3\\] <p>At our input value \\(x = 2\\):</p> \\[\\frac{dy}{dx}\\bigg|_{x=2} = 2(2) + 3 = 7\\] <p>Now let's compute the gradient using Tinygrad's automatic differentiation:</p> <pre><code>grad, = loss.gradient(x)\nprint(grad.item()) # 7.0\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#chain-rule","title":"Chain Rule","text":"<p>Tinygrad automatically applies the chain rule:</p> <pre><code>x = Tensor([2.0], requires_grad=True)\nz = (x**2 + 1).log()\nloss = z.sum()\n\nprint(x.item()) # 2.0\nprint(z.item()) # log(2^2 + 1) = log(5) \u2248 1.6094378232955933\n</code></pre> <p>For the composite function \\(z = \\log(x^2 + 1)\\), we need to apply the chain rule:</p> <ol> <li> <p>Let \\(z = \\log(x^{2}+1)\\).    Set \\(g(x)=x^{2}+1\\). Then \\(z = \\log(g(x))\\).</p> </li> <li> <p>Apply the chain rule: \\(\\frac{dz}{dx} = \\frac{1}{g(x)} \\cdot g'(x) = \\frac{1}{x^2+1} \\cdot 2x = \\frac{2x}{x^2+1}\\)</p> </li> <li> <p>Evaluate at \\(x=2\\): \\(\\frac{dz}{dx}|_{x=2} = \\frac{2 \\cdot 2}{2^2+1} = \\frac{4}{5} = 0.8\\)</p> </li> </ol> <p>Tinygrad handles all the chain rule complexity automatically:</p> <pre><code>grad, = loss.gradient(x)\nprint(grad.item()) # 0.8\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#graph-optimization-and-kernel-generation","title":"Graph Optimization and Kernel Generation","text":"<p>Tinygrad's power lies in its graph rewrite system. Let's see optimization in action:</p> <pre><code>t = Tensor([1, 2, 3, 4])\nt_plus_3_plus_4 = t + 3 + 4\n\nprint(t_plus_3_plus_4.uop)  # Shows addition of 3 and 4 separately\n</code></pre> <pre><code>flowchart LR\n    A[\"ADD (outer)&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    B[\"ADD (inner)&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    C[\"COPY&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    D[\"BUFFER&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n    E[\"UNIQUE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=8\"]\n    F[\"DEVICE&lt;br/&gt;dtypes.void&lt;br/&gt;arg='PYTHON'\"]\n    G[\"DEVICE (x5)&lt;br/&gt;dtypes.void&lt;br/&gt;arg='CPU'\"]\n    H[\"EXPAND&lt;br/&gt;dtypes.int&lt;br/&gt;arg=(4,)\"]\n    I[\"RESHAPE&lt;br/&gt;dtypes.int&lt;br/&gt;arg=(1,)\"]\n    J[\"CONST&lt;br/&gt;dtypes.int&lt;br/&gt;arg=3\"]\n    K[\"VIEW (x9)&lt;br/&gt;dtypes.void&lt;br/&gt;arg=ShapeTracker\"]\n    L[\"EXPAND&lt;br/&gt;dtypes.int&lt;br/&gt;arg=(4,)\"]\n    M[\"RESHAPE&lt;br/&gt;dtypes.int&lt;br/&gt;arg=(1,)\"]\n    N[\"CONST&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n\n    A --&gt; B\n    A --&gt; L\n    B --&gt; C\n    B --&gt; H\n    C --&gt; D\n    C --&gt; G\n    D --&gt; E\n    D --&gt; F\n    H --&gt; I\n    I --&gt; J\n    J --&gt; K\n    K --&gt; G\n    L --&gt; M\n    M --&gt; N\n    N --&gt; K</code></pre> <p>The initial UOP tree shows two separate ADD operations. But when we prepare for kernel generation:</p> <pre><code>t_plus_3_plus_4.kernelize()\nprint(t_plus_3_plus_4.uop)\n</code></pre> <p>The graph rewrite engine has optimized this to add 7 directly, demonstrating constant folding:</p> <pre><code>flowchart LR\n    A[\"ASSIGN (outer)&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    B[\"BUFFER (x0)&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n    C[\"UNIQUE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=10\"]\n    D[\"DEVICE (x2)&lt;br/&gt;dtypes.void&lt;br/&gt;arg='CPU'\"]\n    E[\"KERNEL 12&lt;br/&gt;dtypes.void&lt;br/&gt;arg=&amp;lt;SINK __add__&amp;gt;\"]\n    F[\"ASSIGN (inner)&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    G[\"BUFFER (x5)&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n    H[\"UNIQUE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=9\"]\n    I[\"KERNEL 5&lt;br/&gt;dtypes.void&lt;br/&gt;arg=&amp;lt;COPY&amp;gt;\"]\n    J[\"BUFFER&lt;br/&gt;dtypes.int&lt;br/&gt;arg=4\"]\n    K[\"UNIQUE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=8\"]\n    L[\"DEVICE&lt;br/&gt;dtypes.void&lt;br/&gt;arg='PYTHON'\"]\n\n    A --&gt; B\n    A --&gt; E\n    B --&gt; C\n    B --&gt; D\n    E --&gt; B\n    E --&gt; F\n    F --&gt; G\n    F --&gt; I\n    G --&gt; H\n    G --&gt; D\n    I --&gt; G\n    I --&gt; J\n    J --&gt; K\n    J --&gt; L</code></pre> <p>We can extract and examine the kernel AST:</p> <pre><code>kernel_ast = t_plus_3_plus_4.uop.src[1].arg.ast\n\nfrom tinygrad.codegen import full_rewrite_to_sink\nrewritten_ast = full_rewrite_to_sink(kernel_ast)\nprint(rewritten_ast)\n</code></pre> <pre><code>flowchart LR\n    A[\"SINK&lt;br/&gt;dtypes.void&lt;br/&gt;arg=None\"]\n    B[\"STORE&lt;br/&gt;dtypes.void&lt;br/&gt;arg=None\"]\n    C[\"INDEX (output)&lt;br/&gt;dtypes.int.ptr(4)&lt;br/&gt;arg=None\"]\n    D[\"DEFINE_GLOBAL&lt;br/&gt;dtypes.int.ptr(4)&lt;br/&gt;arg=0\"]\n    E[\"SPECIAL (x3)&lt;br/&gt;dtypes.int&lt;br/&gt;arg=('gidx0', 4)\"]\n    F[\"ADD&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    G[\"LOAD&lt;br/&gt;dtypes.int&lt;br/&gt;arg=None\"]\n    H[\"INDEX (input)&lt;br/&gt;dtypes.int.ptr(4)&lt;br/&gt;arg=None\"]\n    I[\"DEFINE_GLOBAL&lt;br/&gt;dtypes.int.ptr(4)&lt;br/&gt;arg=1\"]\n    J[\"CONST&lt;br/&gt;dtypes.int&lt;br/&gt;arg=7\"]\n\n    A --&gt; B\n    B --&gt; C\n    B --&gt; F\n    C --&gt; D\n    C --&gt; E\n    F --&gt; G\n    F --&gt; J\n    G --&gt; H\n    H --&gt; I\n    H --&gt; E</code></pre> <p>This shows the final optimized computation graph that will be compiled to device code.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#generated-code","title":"Generated Code","text":"<p>With <code>DEBUG=4</code>, we can see the actual generated kernel code:</p> <pre><code>void E_4n2(int* restrict data0, int* restrict data1) {\n  int val0 = *(data1+0);\n  int val1 = *(data1+1);\n  int val2 = *(data1+2);\n  int val3 = *(data1+3);\n  *(data0+0) = (val0+7);\n  *(data0+1) = (val1+7);\n  *(data0+2) = (val2+7);\n  *(data0+3) = (val3+7);\n}\n</code></pre> <p>When running with <code>DEBUG=2</code>, the <code>4</code> appears in yellow to indicate it's been upcasted. Running with <code>NOOPT=1</code> will show the unoptimized code with a loop:</p> <pre><code>void E_4n2(int* restrict data0, int* restrict data1) {\n  for (int ridx0 = 0; ridx0 &lt; 4; ridx0++) {\n    int val0 = *(data1+ridx0);\n    *(data0+ridx0) = (val0+7);\n  }\n}\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#development-and-debugging-tools","title":"Development and Debugging Tools","text":""},{"location":"blog/2025/06/10/introduction-to-tinygrad/#debug-flags","title":"Debug Flags","text":"<p>Tinygrad provides extensive debugging capabilities:</p> <ul> <li><code>DEBUG=2</code> - Shows data movement and kernel execution</li> <li><code>DEBUG=4</code> - Shows generated kernel code</li> <li><code>VIZ=1</code> - Launches web-based graph rewrite explorer</li> <li><code>NOOPT=1</code> - Disables optimizations for debugging</li> </ul>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#color-coding-system","title":"Color Coding System","text":"<p>Tinygrad uses a color system in debug output to indicate optimization states:</p> <ul> <li>Blue: Global operations (e.g. <code>ADD</code>)</li> <li>Light Blue: Local operations (e.g. <code>COPY</code>)</li> <li>Red: Reduce operations (e.g. <code>SUM</code>)</li> <li>Yellow: Upcasted operations (e.g. <code>EXPAND</code>)</li> <li>Purple: Unrolled operations (e.g. <code>UNROLL</code>)</li> <li>Green: Group operations (e.g. <code>GROUP</code>)</li> </ul>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#visual-graph-explorer","title":"Visual Graph Explorer","text":"<p>You can explore Tinygrad's graph rewriting visually:</p> <pre><code>VIZ=1 python ramp.py\n</code></pre> <p>This launches a web-based graph rewrite explorer where you can see UOP transformations in real-time.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#advanced-topics","title":"Advanced Topics","text":""},{"location":"blog/2025/06/10/introduction-to-tinygrad/#low-level-uop-construction","title":"Low-Level UOP Construction","text":"<p>For complete understanding, you can construct UOPs directly:</p> <pre><code>from tinygrad import dtypes\nfrom tinygrad.uop import UOp, Ops\n\n# Create constant UOPs\na = UOp(Ops.CONST, dtypes.int, arg=2)\nb = UOp(Ops.CONST, dtypes.int, arg=2)\n\n# Global uniqueness: same specifications = same object\nassert a is b\n\n# Construct computation\na_plus_b = a + b\nprint(a_plus_b)\n</code></pre> <p>This results in a UOP that adds two constants:</p> <pre><code>UOp(Ops.ADD, dtypes.int, arg=None, src=(\n  x0:=UOp(Ops.CONST, dtypes.int, arg=2, src=()),\n   x0,))\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#pattern-matching-and-graph-rewriting","title":"Pattern Matching and Graph Rewriting","text":"<p>Tinygrad's graph rewrite engine uses pattern matching for optimizations:</p> <pre><code>from tinygrad.uop.ops import graph_rewrite, UPat, PatternMatcher\nfrom tinygrad.uop import UOp, Ops\n\n# Define a constant folding pattern\nsimple_pm = PatternMatcher([\n  (UPat(Ops.ADD, src=(UPat(Ops.CONST, name=\"c1\"), UPat(Ops.CONST, name=\"c2\"))),\n   lambda c1,c2: UOp(Ops.CONST, dtype=c1.dtype, arg=c1.arg+c2.arg)),\n])\n\n# Apply the pattern\na_plus_b_simplified = graph_rewrite(a_plus_b, simple_pm)\nprint(a_plus_b_simplified)  # UOp(Ops.CONST, dtypes.int, arg=4, src=())\n</code></pre> <p>Syntactic sugar makes patterns more readable:</p> <pre><code>simpler_pm = PatternMatcher([\n  (UPat.cvar(\"c1\")+UPat.cvar(\"c2\"), lambda c1,c2: c1.const_like(c1.arg+c2.arg))\n])\n\nassert graph_rewrite(a_plus_b, simple_pm) is graph_rewrite(a_plus_b, simpler_pm)\n</code></pre>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#performance-and-training","title":"Performance and Training","text":""},{"location":"blog/2025/06/10/introduction-to-tinygrad/#performance-expectations","title":"Performance Expectations","text":"<p>You should expect competitive performance today. With <code>BEAM=2</code> optimization, Tinygrad often outperforms PyTorch on:</p> <ul> <li>Unoptimized workloads</li> <li>AMD hardware</li> <li>Training scenarios (20% faster than PyTorch on AMD in HLBC implementation)</li> </ul>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#training-philosophy","title":"Training Philosophy","text":"<p>Tinygrad avoids the <code>trainer.fit()</code> abstraction. If you examine <code>examples/beautiful_mnist.py</code>, you'll find a complete MNIST trainer that contains everything you need and nothing you don't.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#visual-graph-explorer_1","title":"Visual Graph Explorer","text":"<p>You can explore Tinygrad's graph rewriting visually:</p> <pre><code>VIZ=1 python ramp.py\n</code></pre> <p>Environment variables like <code>DEBUG=2</code> or <code>CPU=1</code> are set in bash like: <code>DEBUG=2 CPU=1 python docs/ramp.py</code>.</p> <p>This launches a web-based graph rewrite explorer where you can see UOP transformations in real-time.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#conclusion","title":"Conclusion","text":"<p>You now understand Tinygrad's core concepts: how UOPs transform through graph rewrites to create optimized computational kernels. The beauty lies not in each line being simple, but in the whole system being maintainable by small groups of smart engineers, and you can be one of them.</p>"},{"location":"blog/2025/06/10/introduction-to-tinygrad/#glossary","title":"Glossary","text":"Term Meaning Device Hardware backend where tensors live and kernels run (e.g. <code>CPU</code>, <code>CUDA</code>, <code>METAL</code>). Lazy / Realize Tinygrad records operations lazily; <code>realize()</code> forces execution. UOp Immutable micro-operation node in the computation graph. DAG Directed acyclic graph composed of UOps. Kernelize Transformation that groups UOps into executable kernels. ShapeTracker Tracks tensor views/shapes without copying data. Upcast Optimisation that widens ops to work on multiple elements at once."},{"location":"projects/graham/","title":"Graham Essays","text":"<p>A comprehensive digital archive of Paul Graham's influential writings, optimized for offline reading in EPUB and Markdown</p> <p>Featured on Hacker News front page at launch, check on GitHub for more details.</p> <p></p>"},{"location":"projects/graham/#overview","title":"Overview","text":"<p>This project automatically downloads, processes, and formats the complete collection of essays from Paul Graham's website into convenient digital formats. What started as a simple web scraping exercise turned into a substantial digital library containing over 200 essays and more than 500,000 words of influential writing on startups, technology, and entrepreneurship.</p>"},{"location":"projects/graham/#key-features","title":"Key Features","text":"<ul> <li>Complete &amp; Current: The entire collection is automatically rebuilt and updated daily via GitHub Actions. This project was featured on the Hacker News front page and has received over 800 GitHub stars.</li> <li>Multiple Formats: Available in both EPUB (for e-readers) and Markdown (for developers/note-takers).</li> <li>Optimized for Reading: Clean, well-formatted text for a high-quality reading experience on any device.</li> <li>Offline Access: Download once, read anywhere without an internet connection.</li> <li>Open Source: Full source code is available for transparency and community-driven development.</li> </ul>"},{"location":"projects/graham/#technical-implementation","title":"Technical Implementation","text":"<p>The project leverages the RSS feed originally created by Aaron Swartz and officially shared by Paul Graham himself, ensuring legitimate access. The automation pipeline uses several Python libraries for clean, readable output:</p> <ul> <li><code>feedparser</code> - Parses the RSS feed to retrieve essay metadata</li> <li><code>html2text</code> - Converts HTML content to a clean Markdown format</li> <li><code>htmldate</code> - Accurately extracts publication dates from web pages</li> <li><code>Unidecode</code> - Handles character encoding for consistent text formatting</li> </ul>"},{"location":"projects/graham/#getting-started","title":"Getting Started","text":""},{"location":"projects/graham/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/ofou/graham-essays.git\ncd graham-essays\nmake\n</code></pre> <p>This generates fresh EPUB and Markdown files with the latest essays.</p>"},{"location":"projects/graham/#downloads","title":"Downloads","text":"<p>Ready-to-use files (updated daily via GitHub Actions):</p> <ul> <li>\ud83c\udff7\ufe0f All Releases</li> <li>\ud83d\udcda Direct EPUB Download</li> <li>\ud83d\udcc4 Essay Index (CSV)</li> </ul>"},{"location":"projects/graham/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Whether it's code improvements, format suggestions, or feature requests:</p> <ul> <li>Open an issue for bug reports or feature requests</li> <li>Submit a pull request for code improvements</li> <li>Share feedback on the reading experience</li> </ul> <p>The goal is to make Paul Graham's essays as accessible as possible while maintaining the highest quality reading experience.</p> <p>This is an independent project created for educational and personal use. All essay content remains the intellectual property of Paul Graham.</p>"},{"location":"projects/textube/","title":"Textube: YouTube-to-ChatGPT API","text":"<p>Built before Google integrated YouTube into Gemini natively :)</p> <p>Featured on Hacker News, currently live at textube, ChatGPT Plugin or GitHub</p>"},{"location":"projects/textube/#the-problem-i-solved","title":"The Problem I Solved","text":"<p>Before Google built YouTube integration directly into Gemini, there was no clean way to analyze YouTube videos with LLMs. You could copy-paste transcripts manually, but that was tedious and broke for long-form content. I built Textube to bridge this gap - a clean API that lets ChatGPT analyze any YouTube video or entire playlists.</p>"},{"location":"projects/textube/#technical-architecture","title":"Technical Architecture","text":"<p>FastAPI backend with two main endpoints:</p> <ul> <li><code>/watch?v={video_id}&amp;format={json|txt}</code> - Individual video transcripts mirroring the YouTube API</li> <li><code>/playlist?list={playlist_id}</code> - Process entire playlists with metadata mirroring the YouTube API</li> </ul> <p>Core Components:</p> <pre><code>class Captions:\n    def __init__(self, video_id: str):\n        # Uses youtube-transcript-api to fetch transcripts\n        # Calculates token counts with tiktoken for ChatGPT context limits\n        # Processes timestamps and metadata\n</code></pre> <pre><code>class Playlist:\n    def process_playlist(self) -&gt; None:\n        # Batch processes hundreds of videos\n        # Maintains video order and calculates aggregate statistics\n        # Handles playlist-level caching and integrity checks\n</code></pre> <p>Data Flow:</p> <ol> <li>Extract video/playlist IDs from YouTube URLs using regex</li> <li>Fetch transcripts via YouTube's transcript API</li> <li>Process and structure the data (timestamps, token counts, metadata)</li> <li>Cache in Google Cloud Firestore for performance</li> <li>Serve as JSON/TXT or integrate with ChatGPT plugin</li> </ol>"},{"location":"projects/textube/#key-technical-decisions","title":"Key Technical Decisions","text":"<p>Caching Strategy: Started with local JSON files, migrated to Firestore as usage scaled</p> <pre><code># Original approach - local caching\nvideo_file = os.path.join(self.base_path, self.playlist_id, f\"{video_id}.json\")\n\n# Scaled approach - Firestore with document references\nvideo_ref = db.collection(\"videos\").document(video_id)\n</code></pre> <p>Token Counting: Used <code>tiktoken</code> to calculate exact token counts for ChatGPT context windows</p> <pre><code>def _count_tokens(self, model_name: str = \"gpt-4\") -&gt; int:\n    encoding = tiktoken.encoding_for_model(model_name)\n    return len(encoding.encode(str(self)))\n</code></pre> <p>Playlist Processing: Built to handle massive playlists (Lex Fridman's 250+ videos, Dwarkesh's 72 videos = 1M+ words)</p> <ul> <li>Maintains video order through document references</li> <li>Calculates aggregate statistics (total tokens, characters, duration)</li> <li>Handles partial failures gracefully</li> </ul> <p>ChatGPT Integration: Implemented as OpenAI plugin with proper API specification</p> <ul> <li>Exposes transcript data directly to ChatGPT's context</li> <li>Handles large transcripts through chunking strategies</li> <li>Maintains conversation context across video segments</li> </ul>"},{"location":"projects/textube/#tech-stack","title":"Tech Stack","text":"<pre><code>Backend: FastAPI + Python\nDatabase: Google Cloud Firestore\nAPIs: youtube-transcript-api, pytubefix\nToken Counting: tiktoken\nFrontend: Vanilla JS + Tailwind CSS\nDeployment: Docker container\nIntegration: OpenAI Plugin API\n</code></pre>"},{"location":"projects/textube/#performance-characteristics","title":"Performance Characteristics","text":"<p>Handles edge cases:</p> <ul> <li>8-hour podcasts (Lex Fridman)</li> <li>Multi-language transcripts (English, Spanish fallbacks)</li> <li>Missing/corrupted transcript data</li> <li>Rate limiting and API failures</li> </ul> <p>Caching effectiveness:</p> <ul> <li>Instant responses for previously processed videos</li> <li>Playlist-level caching with integrity verification</li> <li>Graceful fallback to YouTube API when cache misses</li> </ul>"},{"location":"projects/textube/#open-source-proof-of-concept","title":"Open Source Proof of Concept","text":"<p>Released as open source on GitHub to demonstrate the concept and let others build upon it. The goal was to validate demand for YouTube-LLM integration and show it was technically feasible.</p> <p>Overwhelming demand: The service got so much usage that Google eventually banned it for exceeding API limits. This was actually validation - the demand was real enough that Google had to take action.</p> <p>Strategic caching: Before the ban, I managed to cache thousands of popular videos and entire playlists (Karpathy's courses, 3Blue1Brown, Lex Fridman, etc.). These cached examples still work and demonstrate the concept.</p> <p>Multiple interfaces:</p> <ul> <li>Direct API access for developers: <code>GET /watch?v={video_id}&amp;format=json</code></li> <li>ChatGPT plugin for conversational analysis: Integrated via OpenAI's plugin system</li> <li>Simple web interface for quick access: Clean HTML form with real-time results</li> </ul> <p>Check the API docs for more details. Here are some examples:</p> <pre><code>curl https://textube.olivares.cl/watch?v=bZQun8Y4L2A&amp;format=json | jq # State of GPT by Andrej Karpathy\n</code></pre> <p>Response:</p> <pre><code>{\n  \"character_count\": 43265,\n  \"title\": \"State of GPT | BRK216HFS\",\n  \"url\": \"https://www.youtube.com/watch?v=bZQun8Y4L2A\",\n  \"video_id\": \"bZQun8Y4L2A\",\n  \"token_count\": 9975,\n  \"transcript\": [\n    {\n      \"end\": 7.159,\n      \"start\": 0.235,\n      \"text\": \"[MUSIC]\"\n    },\n    {\n      \"end\": 8.896,\n      \"start\": 7.159,\n      \"text\": \"ANNOUNCER:\\nPlease welcome\"\n    },\n    {\n      \"end\": 10.231,\n      \"start\": 8.896,\n      \"text\": \"AI researcher and\"\n    },\n    {\n      \"end\": 21.169,\n      \"start\": 10.231,\n      \"text\": \"founding member of\\nOpenAI, Andrej Karpathy.\"\n    },\n    ...\n  ]\n}\n</code></pre> <pre><code>curl https://textube.olivares.cl/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ | jq # Neural Networks: Zero to Hero playlist\n</code></pre> <p>Response:</p> <pre><code>{\n  \"playlist_id\": \"PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\",\n  \"title\": \"Neural Networks: Zero to Hero\",\n  \"url\": \"https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\",\n  \"total_token_count\": 222771,\n  \"total_character_count\": 1066072,\n  \"video_count\": 10,\n  \"videos\": [\n    {\n      \"video_id\": \"VMj-3S1tku0\",\n      \"title\": \"The spelled-out intro to neural networks and backpropagation: building micrograd\",\n      \"token_count\": 26238,\n      \"character_count\": 123212,\n      \"local_link\": \"https://textube.olivares.cl/watch?v=VMj-3S1tku0\"\n    },\n    {\n      \"video_id\": \"PaCmpygFfXo\",\n      \"title\": \"The spelled-out intro to language modeling: building makemore\",\n      \"token_count\": 20811,\n      \"character_count\": 100179,\n      \"local_link\": \"https://textube.olivares.cl/watch?v=PaCmpygFfXo\"\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"projects/textube/#why-it-hit-hacker-news-frontpage","title":"Why It Hit Hacker News Frontpage","text":"<p>Timing: Built this before major LLM providers integrated YouTube natively. Solved a real pain point that thousands of developers/researchers had.</p> <p>Immediate utility: People could paste any YouTube URL and start analyzing the content with ChatGPT within seconds. No complex setup, no API keys.</p> <p>Scale: Handled viral traffic from HN without breaking. The Firestore caching strategy paid off when hundreds of users started processing large playlists.</p> <p>Community validation: HN commenters immediately understood the value and started sharing their own use cases - research, content analysis, learning from technical talks.</p>"},{"location":"projects/textube/#technical-impact","title":"Technical Impact","text":"<p>This project validated several architectural patterns I've used in subsequent projects:</p> <p>API-first design: Clean separation between data processing (captions.py) and API layer (app.py)</p> <p>Progressive enhancement: Started simple (single videos) then expanded to complex use cases (playlists, bulk processing)</p> <p>Caching as a first-class concern: Moved from local files to cloud database as usage patterns became clear</p> <p>LLM integration patterns: Token counting, context window management, and plugin architectures that became standard later</p>"},{"location":"projects/textube/#from-banned-to-built-in","title":"From Banned to Built-in","text":"<p>The trajectory tells the story: Google banned the service due to demand, then months later built similar functionality directly into Gemini. Classic pattern of successful proof-of-concepts getting killed then rebuilt natively.</p> <p>Open source legacy: The code remains available as a reference implementation. Shows how to:</p> <ul> <li>Build LLM integrations before official APIs exist</li> <li>Handle scale with smart caching strategies</li> <li>Design APIs that work with both humans and AI systems</li> <li>Process large-scale content for AI consumption</li> </ul> <p>Validation through prohibition: Sometimes getting banned is the strongest product-market fit signal. The demand was real enough that Google had to intervene.</p> <p>The lesson: build proof-of-concepts that push boundaries. Even if they get shut down, they demonstrate market demand and technical feasibility that larger companies eventually adopt.</p> <p>Open source on GitHub. Cached examples still work. Can be used via direct API calls or ChatGPT integration.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/machine-learning/","title":"Machine Learning","text":""},{"location":"blog/category/books/","title":"Books","text":""}]}