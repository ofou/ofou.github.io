{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello, I'm Omar","text":"<p>I'm a Software Engineer with a passion for Artificial Intelligence (AI) and Machine Learning (ML) who likes to turn complex ideas into practical applications. I previously worked as a music producer, content creator for Neura Pod, then pivoted into AI Engineering and Developer Advocate, building AI-powered products like Kilo Code coding agent, Emergent Mind RAG systems for CS research, and scalable web applications for various startups. I'm currently available for hiring, consulting, or collaboration.</p>"},{"location":"#what-i-do","title":"What I Do","text":"<ol> <li>Build production-ready AI/ML products</li> <li>Consulting and training in AI/ML for startups and enterprises</li> <li>Create technical content from internal docs to videos</li> <li>Independent research in AI, neuroscience, and machine learning</li> </ol> <p>I also enjoy making music, contributing to open-source projects, learning Mandarin (\u4f60\u597d), and exploring new places around the world.</p>"},{"location":"#lets-connect","title":"Let's Connect","text":"<p>Reach out via X, LinkedIn, or omar@olivares.cl to discuss your next project or just to chat about technology.</p>"},{"location":"blog/","title":"Blog","text":"<p>This is my blog. I write about my experiences, thoughts, and ideas.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/","title":"Jiro Dreams of Sushi","text":"<p>I just watched Jiro Dreams of Sushi, and I was really amazed by the quality of the documentary. I resonate highly with Jiro's philosophy and work ethic, and I think that his story is a great reminder that anyone can achieve greatness if they're willing to put in the time and effort. I think his insights are applicable to anyone in any field, not just sushi chefs.</p> <p>Here's a collection of quotes from a follow-up interview with Jiro and his son, Yoshikazu. Now that everyone is trying to cook at work, I think these insights are more relevant than ever.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-loving-the-work","title":"On Loving the Work","text":"<p>You must come to love your job \u2014 otherwise, it's no good.</p> <p>If you start saying things like, 'I don't like this,' or 'This isn't for me,' you'll never become a master.</p> <p>Once you've chosen your path, you move forward. Forward, forward.</p> <p>Young people today \u2014 they talk like they're already masters. But when it comes to real work, they fall short. Second-rate. Third-rate.</p> <p>That's because machines have advanced. Everyone rushes to what's easy.</p> <p>But the real work \u2014 the hand work \u2014 like sushi, or any craft done by hand\u2026 fewer and fewer people are doing it now. Especially the young.</p> <p>If they don't love it, they won't improve.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-innovation","title":"On Innovation","text":"<p>If something new tastes better \u2014 that's fine.</p> <p>But if it just looks beautiful and the flavor declines, it won't last.</p> <p>Flavor must always come first.</p> <p>He told me about octopus \u2014 tako.</p> <p>In the past, most people didn't like it.</p> <p>But I changed the way I prepared it. Improved it.</p> <p>And then, the people who used to say 'I hate tako' started asking for seconds.</p> <p>That's when you know you've got real skill.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-mastery","title":"On Mastery","text":"<p>I asked: When did you feel like a true craftsman? Like a master?</p> <p>Around 50.</p> <p>Fifty?</p> <p>Yes. Five-zero.</p> <p>Before that? You fail. Then improve. Fail again. Improve again.</p> <p>Around age 50, you start to finally create something close to what you have in mind.</p> <p>From eight years old until 50 \u2014 did he ever want to quit?</p> <p>Never. Not once.</p> <p>It was never, 'Should I quit?'</p> <p>It was always, 'How can I get better?'</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-family-and-craft","title":"On Family and Craft","text":"<p>What's it like working alongside your son all these years?</p> <p>Inside the restaurant, I don't think of him as my son.</p> <p>If I did, he wouldn't improve.</p> <p>He's a craftsman. I'm the master. Simple as that.</p> <p>His son spoke up:</p> <p>I think I'm lucky. Having a teacher like this by my side.</p> <p>But I only started feeling that way recently.</p> <p>He was strict. It was tough.</p> <p>How long has he been doing this?</p> <p>This is my 37<sup>th</sup> or 38<sup>th</sup> year.</p> <p>But I haven't even reached half his level.</p> <p>Every time I feel like I'm catching up, I look up\u2026</p> <p>\u2026and he's already a thousand steps ahead.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-the-old-days","title":"On the Old Days","text":"<p>Back then, you couldn't just say, 'I'll go here' or 'I'll quit and try that.'</p> <p>There were too many mouths to feed.</p> <p>If someone took you in, that was enough. You stayed, studied hard, and earned your place.</p> <p>Did he feel competitive with his peers?</p> <p>Not 'I won't lose' \u2014 I thought, 'I will overtake them.'</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-happiness-and-purpose","title":"On Happiness and Purpose","text":"<p>What brings more happiness \u2014 making sushi or making people happy?</p> <p>Being able to work is happiness.</p> <p>Being able to do work you love \u2014 that's a higher level of happiness.</p> <p>It's all about accumulation.</p> <p>I'm still working at 90. Work is my ikigai \u2014 my reason for living.</p> <p>He told me about a time someone asked if he'd do something fun over a three-day holiday:</p> <p>I said, 'If I don't get to work soon, I'll die of boredom.'</p> <p>I'd rather be at the restaurant.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-legacy","title":"On Legacy","text":"<p>At 60 or 70, you've probably achieved what you aimed for.</p> <p>But you don't quit.</p> <p>You think, What's next?</p> <p>That's how humans are.</p> <p>I nodded. I think I'm starting to understand.</p> <p>You're probably already thinking, 'What's next? Should I keep going?'</p> <p>But when you've accomplished something, the question becomes \u2014</p> <p>What else can I do?</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-struggle-and-will","title":"On Struggle and Will","text":"<p>We talked about Ren\u00e9 Redzepi, about struggle, and stubbornness.</p> <p>You have to be strong-willed and sensitive.</p> <p>That's how you get here.</p> <p>And those at the top?</p> <p>They don't retire at 70 or 80.</p> <p>They tighten their belts.</p> <p>The next accomplishment is even closer.</p> <p>This will happen to you too, I bet.</p> <p>But if I'm alive when it does, I'll say \u2014 See? I told you so.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#one-more","title":"One More?","text":"<p>I asked if this was the end.</p> <p>He laughed.</p> <p>One more? Yes. One more.</p> <p>The not-so-secret dream: to make sushi until the Tokyo Olympics.</p> <p>He would be 95.</p> <p>During the last Olympics, I was already here.</p> <p>I want to be here for the next one, too.</p> <p>Thank you for the meal.</p>"},{"location":"blog/2024/01/01/learning-in-2025/","title":"Things I want to learn in 2025","text":"<p>This year, my focus is on exploring the intersection of neuroscience and AI. I'll be diving deep into textbooks and hands-on projects to better understand both the human brain and artificial intelligence. The following lists outline the books, textbooks, and codebases that I'm currently reading and coding from.</p> <pre><code>graph TD\n    %% Mathematical Foundations (Base Layer)\n    math[\"Mathematics for Machine Learning\"]\n    stats[\"An Introduction to Statistical Learning\"]\n    prob[\"Probabilistic Machine Learning\"]\n    bayes[\"Bayesian Reasoning and Machine Learning\"]\n\n    %% Core AI Foundations\n    aimodern[\"AI: A Modern Approach\"]\n    aihistory[\"AI: From Medieval Robots to Neural Networks\"]\n    profiles[\"Profiles of the Future\"]\n\n    %% Learning &amp; Neuroscience\n    howlearn[\"How We Learn\"]\n    artlearn[\"Art of Doing Science and Engineering\"]\n\n    %% ML Fundamentals\n    mlintro[\"Introduction to Machine Learning\"]\n    hundred[\"The Hundred-Page ML Book\"]\n    grokking[\"Grokking Machine Learning\"]\n    foundations[\"Foundations of Machine Learning\"]\n\n    %% Deep Learning Theory\n    littlebook[\"The Little Book of Deep Learning\"]\n    dlconcepts[\"Deep Learning: Foundations and Concepts\"]\n    dlgoodfellow[\"Deep Learning (Goodfellow)\"]\n    understanding[\"Understanding Deep Learning\"]\n    principles[\"Principles of Deep Learning Theory\"]\n    neural[\"Neural Networks and Deep Learning\"]\n\n    %% Practical Deep Learning\n    dlcoders[\"Deep Learning for Coders with Fastai\"]\n    dlscratch[\"Deep Learning from Scratch\"]\n    nnpython[\"Neural Networks from Scratch in Python\"]\n    dlpython[\"Deep Learning with Python\"]\n    pytorch[\"Machine Learning with PyTorch and Scikit-Learn\"]\n    dive[\"Dive into Deep Learning\"]\n\n    %% AI Engineering &amp; Systems\n    aieng[\"AI Engineering: Building Applications\"]\n    mlsystems[\"Designing Machine Learning Systems\"]\n\n    %% AI Safety &amp; Ethics\n    aisafety[\"Introduction to AI Safety, Ethics and Society\"]\n    alignment[\"The Alignment Problem\"]\n    compatible[\"Human Compatible\"]\n\n    %% AI Future &amp; Philosophy\n    paths[\"Paths, Dangers, Strategies\"]\n    life3[\"Life 3.0\"]\n    worlds[\"The Worlds I See\"]\n    singularity[\"The Singularity Is Nearer\"]\n\n    %% Specialized Topics\n    rl[\"Reinforcement Learning: An Introduction\"]\n    marl[\"Multi-agent Reinforcement Learning\"]\n    parallel[\"Programming Massively Parallel Processors\"]\n\n    %% Code &amp; Frameworks\n    python[\"Python\"]\n    pytorchcode[\"PyTorch\"]\n    jax[\"JAX\"]\n    xla[\"XLA\"]\n    transformers[\"Transformers (HuggingFace)\"]\n    mlx[\"MLX\"]\n    mlxexamples[\"MLX Examples\"]\n    axlearn[\"axlearn\"]\n    tinygrad[\"tinygrad\"]\n    ggml[\"ggml\"]\n    triton[\"Triton\"]\n    cuda[\"CUDA\"]\n    gpupuzzles[\"GPU Puzzles\"]\n    nanogpt[\"nanoGPT\"]\n    micrograd[\"micrograd\"]\n    llmc[\"llm.c\"]\n    xtransformers[\"x-transformers\"]\n\n    %% Core Dependencies\n    math --&gt; stats\n    math --&gt; prob\n    math --&gt; bayes\n    math --&gt; mlintro\n    math --&gt; foundations\n\n    profiles --&gt; aihistory\n    aihistory --&gt; aimodern\n    aimodern --&gt; mlintro\n    aimodern --&gt; aisafety\n\n    howlearn --&gt; artlearn\n    artlearn --&gt; mlintro\n\n    %% ML to DL progression\n    mlintro --&gt; hundred\n    hundred --&gt; grokking\n    mlintro --&gt; foundations\n    foundations --&gt; littlebook\n\n    stats --&gt; dlconcepts\n    prob --&gt; dlconcepts\n    bayes --&gt; dlgoodfellow\n\n    littlebook --&gt; understanding\n    dlconcepts --&gt; dlgoodfellow\n    dlgoodfellow --&gt; principles\n    dlgoodfellow --&gt; neural\n    neural --&gt; understanding\n\n    %% Theory to Practice\n    littlebook --&gt; dlcoders\n    neural --&gt; dlscratch\n    neural --&gt; nnpython\n    dlconcepts --&gt; dlpython\n    dlconcepts --&gt; pytorch\n    understanding --&gt; dive\n\n    %% AI Engineering connections\n    aimodern --&gt; aieng\n    dlpython --&gt; aieng\n    mlintro --&gt; mlsystems\n    aieng --&gt; mlsystems\n\n    %% Safety and Future\n    aimodern --&gt; paths\n    paths --&gt; life3\n    paths --&gt; compatible\n    compatible --&gt; alignment\n    aisafety --&gt; alignment\n    aieng --&gt; aisafety\n    howlearn --&gt; worlds\n    life3 --&gt; singularity\n    worlds --&gt; singularity\n\n    %% Specialized topics\n    foundations --&gt; rl\n    rl --&gt; marl\n    dlgoodfellow --&gt; parallel\n\n    %% Code implementations\n    python --&gt; pytorchcode\n    python --&gt; jax\n    pytorchcode --&gt; xla\n    pytorchcode --&gt; transformers\n    pytorchcode --&gt; triton\n    jax --&gt; xla\n\n    dlscratch --&gt; micrograd\n    micrograd --&gt; tinygrad\n    nnpython --&gt; micrograd\n\n    dlcoders --&gt; nanogpt\n    transformers --&gt; nanogpt\n    nanogpt --&gt; llmc\n    transformers --&gt; xtransformers\n\n    python --&gt; mlx\n    mlx --&gt; mlxexamples\n    mlx --&gt; axlearn\n    jax --&gt; axlearn\n\n    parallel --&gt; cuda\n    cuda --&gt; triton\n    cuda --&gt; gpupuzzles\n    triton --&gt; tinygrad\n\n    dlpython --&gt; ggml\n    ggml --&gt; llmc\n\n    %% Connecting practice to code\n    pytorch --&gt; pytorchcode\n    dlcoders --&gt; pytorchcode\n    dive --&gt; jax\n    dive --&gt; pytorchcode\n\n    %% Ensuring all nodes are connected\n    aieng --&gt; transformers\n    mlsystems --&gt; tinygrad\n    principles --&gt; xtransformers</code></pre>"},{"location":"blog/2024/01/01/learning-in-2025/#books","title":"Books","text":""},{"location":"blog/2024/01/01/learning-in-2025/#ai-history-future","title":"AI History &amp; Future","text":"<ul> <li> Profiles of the Future <sup>1</sup></li> <li> Artificial Intelligence: From Medieval Robots to Neural Networks <sup>2</sup></li> <li> Paths, Dangers, Strategies <sup>3</sup></li> <li> Life 3.0: Being Human in the Age of Artificial Intelligence <sup>4</sup></li> <li> Human Compatible: AI and the Problem of Control <sup>5</sup></li> <li> The Alignment Problem: How Can Machines Learn Human Values? <sup>6</sup></li> <li> The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn of AI <sup>7</sup></li> <li> The Singularity Is Nearer: When We Merge with AI <sup>8</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#neuroscience-learning","title":"Neuroscience &amp; Learning","text":"<ul> <li> How we learn: Why brains learn better than any machine... for now <sup>9</sup></li> <li> Art of Doing Science and Engineering: Learning to Learn <sup>10</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#textbooks","title":"Textbooks","text":""},{"location":"blog/2024/01/01/learning-in-2025/#ai-ml-fundamentals","title":"AI &amp; ML Fundamentals","text":"<ul> <li> AI Engineering: Building Applications with Foundation Models <sup>11</sup></li> <li> Artificial Intelligence: A Modern Approach <sup>12</sup></li> <li> Introduction to AI Safety, Ethics and Society <sup>13</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#deep-learning-theory-practice","title":"Deep Learning Theory &amp; Practice","text":"<ul> <li> The Little Book of Deep Learning <sup>14</sup></li> <li> Deep Learning: Foundations and Concepts <sup>15</sup></li> <li> Neural Networks and Deep Learning <sup>16</sup></li> <li> Deep Learning <sup>17</sup></li> <li> Understanding Deep Learning <sup>18</sup></li> <li> Dive into Deep Learning <sup>19</sup></li> <li> The Principles of Deep Learning Theory <sup>20</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#machine-learning-hands-on","title":"Machine Learning Hands-On","text":"<ul> <li> Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD <sup>21</sup></li> <li> Deep Learning from Scratch: Building with Python from First Principles <sup>22</sup></li> <li> Neural Networks from Scratch in Python: Building Neural Networks in Raw Python <sup>23</sup></li> <li> Deep Learning with Python <sup>24</sup></li> <li> Machine Learning with PyTorch and Scikit-Learn: Develop Machine Learning and Deep Learning Models with Python <sup>25</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#mathematical-foundations","title":"Mathematical Foundations","text":"<ul> <li> An Introduction to Statistical Learning: With Applications in Python <sup>26</sup></li> <li> Mathematics for Machine Learning <sup>27</sup></li> <li> Bayesian Reasoning and Machine Learning <sup>28</sup></li> <li> Probabilistic Machine Learning: An Introduction <sup>29</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#ml-systems-engineering","title":"ML Systems &amp; Engineering","text":"<ul> <li> Designing Machine Learning Systems <sup>30</sup></li> <li> Foundations of Machine Learning <sup>31</sup></li> <li> Introduction to Machine Learning <sup>32</sup></li> <li> The Hundred-Page Machine Learning Book <sup>33</sup></li> <li> Grokking Machine Learning <sup>34</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li> Reinforcement Learning: An Introduction <sup>35</sup></li> <li> Multi-agent Reinforcement Learning: Foundations and Modern Approaches <sup>36</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#high-performance-computing","title":"High-Performance Computing","text":"<ul> <li> Programming Massively Parallel Processors: A Hands-on Approach <sup>37</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#code","title":"Code","text":""},{"location":"blog/2024/01/01/learning-in-2025/#core-frameworks-languages","title":"Core Frameworks &amp; Languages","text":"<ul> <li> Python &amp; PyTorch by Meta, XLA by Google</li> <li> JAX by Google DeepMind <sup>38</sup></li> <li> Transformers <sup>39</sup> by Hugging Face</li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#low-level-performance","title":"Low-Level &amp; Performance","text":"<ul> <li> tinygrad by tiny corp</li> <li> ggml by Georgi Gerganov</li> <li> Triton by OpenAI <sup>40</sup></li> <li> CUDA by Nvidia &amp; GPU Puzzles by Sasha Rush</li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#educational-implementations","title":"Educational Implementations","text":"<ul> <li> nanoGPT, micrograd, &amp; llm.c by Andrej Karpathy</li> <li> x-transfomers by Phil Wang (aka lucidrains)</li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#apple-ecosystem","title":"Apple Ecosystem","text":"<ul> <li> MLX <sup>41</sup>, MLX examples &amp; axlearn by Apple</li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#references","title":"References","text":"<ol> <li> <p>Clarke, A. C. Profiles of the future. (Hachette UK, 1962).\u00a0\u21a9</p> </li> <li> <p>Pickover, C. A. Artificial intelligence: From medieval robots to neural networks. (Union Square+ ORM, 2019).\u00a0\u21a9</p> </li> <li> <p>Bostrom, N. S. Paths, dangers, strategies. (Oxford University Press: Oxford, UK, 2014).\u00a0\u21a9</p> </li> <li> <p>Tegmark, M. Life 3.0: Being human in the age of artificial intelligence. (Vintage, 2018).\u00a0\u21a9</p> </li> <li> <p>Russell, S. Human compatible: AI and the problem of control. (Penguin Uk, 2019).\u00a0\u21a9</p> </li> <li> <p>Christian, B. The alignment problem: How can machines learn human values? (Atlantic Books, 2021).\u00a0\u21a9</p> </li> <li> <p>Li, F.-F. The worlds i see: Curiosity, exploration, and discovery at the dawn of AI. (Flatiron books: a moment of lift book, 2023).\u00a0\u21a9</p> </li> <li> <p>Kurzweil, R. The singularity is nearer: When we merge with AI. (Random House, 2024).\u00a0\u21a9</p> </li> <li> <p>Dehaene, S. How we learn: Why brains learn better than any machine... For now. (Penguin, 2021).\u00a0\u21a9</p> </li> <li> <p>Hamming, R. R. Art of doing science and engineering: Learning to learn. 432 (CRC Press, 1997).\u00a0\u21a9</p> </li> <li> <p>Huyen, C. AI Engineering. (O'Reilly Media, 2025).\u00a0\u21a9</p> </li> <li> <p>Russell, S. J. &amp; Norvig, P. Artificial intelligence: A modern approach. (Pearson, 2020).\u00a0\u21a9</p> </li> <li> <p>Hendrycks, D. Introduction to AI safety, ethics and society. (Dan Hendrycks, 2024).\u00a0\u21a9</p> </li> <li> <p>Fleuret, F. The little book of deep learning. (Fran\u00e7ois Fleuret, 2023).\u00a0\u21a9</p> </li> <li> <p>Bishop, C. M. &amp; Bishop, H. Deep learning: Foundations and concepts. (Springer, 2024).\u00a0\u21a9</p> </li> <li> <p>Nielsen, A. Neural networks and deep learning. (2015).\u00a0\u21a9</p> </li> <li> <p>Goodfellow, I., Bengio, Y. &amp; Courville, A. Deep learning. (MIT press, 2016).\u00a0\u21a9</p> </li> <li> <p>Prince, S. J. D. Understanding deep learning. (The MIT Press, 2023).\u00a0\u21a9</p> </li> <li> <p>Zhang, A., Lipton, Z. C., Li, M. &amp; Smola, A. J. Dive into deep learning. (Cambridge University Press, 2023).\u00a0\u21a9</p> </li> <li> <p>Roberts, D. A., Yaida, S. &amp; Hanin, B. The principles of deep learning theory. (Cambridge University Press Cambridge, MA, USA, 2022).\u00a0\u21a9</p> </li> <li> <p>Howard, J. &amp; Gugger, S. Deep learning for coders with fastai and PyTorch. (O'Reilly Media, 2020).\u00a0\u21a9</p> </li> <li> <p>Weidman, S. Deep learning from scratch: Building with python from first principles. (O'Reilly Media, 2019).\u00a0\u21a9</p> </li> <li> <p>Kinsley, H. &amp; Kukie\u0142a, D. Neural networks from scratch in python: Building neural networks in raw python. (Verlag nicht ermittelbar, 2020).\u00a0\u21a9</p> </li> <li> <p>Chollet, F. Deep learning with python. (Simon; Schuster, 2021).\u00a0\u21a9</p> </li> <li> <p>Raschka, S., Liu, Y. H., Mirjalili, V. &amp; Dzhulgakov, D. Machine learning with PyTorch and scikit-learn: Develop machine learning and deep learning models with python. (Packt Publishing Ltd, 2022).\u00a0\u21a9</p> </li> <li> <p>James, G., Witten, D., Hastie, T., Tibshirani, R. &amp; Taylor, J. An introduction to statistical learning: With applications in python. (Springer Nature, 2023).\u00a0\u21a9</p> </li> <li> <p>Deisenroth, M. P., Faisal, A. A. &amp; Ong, C. S. Mathematics for machine learning. (Cambridge University Press, 2020).\u00a0\u21a9</p> </li> <li> <p>Barber, D. Bayesian reasoning and machine learning. (Cambridge University Press, 2012).\u00a0\u21a9</p> </li> <li> <p>Murphy, K. P. Probabilistic machine learning: An introduction. (MIT press, 2022).\u00a0\u21a9</p> </li> <li> <p>Huyen, C. Designing machine learning systems. (\\\" O'Reilly Media, Inc.\\\", 2022).\u00a0\u21a9</p> </li> <li> <p>Mohri, M., Rostamizadeh, A. &amp; Talwalkar, A. Foundations of machine learning. (MIT press, 2018).\u00a0\u21a9</p> </li> <li> <p>Alpaydin, E. Introduction to machine learning. (MIT press, 2020).\u00a0\u21a9</p> </li> <li> <p>Burkov, A. The hundred-page machine learning book. (Andriy Burkov Quebec City, QC, Canada, 2019).\u00a0\u21a9</p> </li> <li> <p>Serrano, L. Grokking machine learning. (Simon; Schuster, 2021).\u00a0\u21a9</p> </li> <li> <p>Andrew, B. &amp; Richard S, S. Reinforcement learning: An introduction. (The MIT Press, 2018).\u00a0\u21a9</p> </li> <li> <p>Albrecht, S. V., Christianos, F. &amp; Sch\u00e4fer, L. Multi-agent reinforcement learning: Foundations and modern approaches. (MIT Press, 2024).\u00a0\u21a9</p> </li> <li> <p>Kirk, D. B. &amp; Hwu, W. W. Programming massively parallel processors: A hands-on approach. (Morgan Kaufmann, 2016).\u00a0\u21a9</p> </li> <li> <p>Bradbury, J. et al. JAX: Composable transformations of Python+NumPy programs. (2018).\u00a0\u21a9</p> </li> <li> <p>Wolf, T. et al. Transformers: State-of-the-art natural language processing. in Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations 38--45 (Association for Computational Linguistics, 2020).\u00a0\u21a9</p> </li> <li> <p>Tillet, P., Kung, H.-T. &amp; Cox, D. Triton: An intermediate language and compiler for tiled neural network computations. 10--19 (2019).\u00a0\u21a9</p> </li> <li> <p>Awni, A. et al. MLX. (2023).\u00a0\u21a9</p> </li> </ol>"},{"location":"projects/graham/","title":"Graham Essays","text":"<p>A comprehensive digital archive of Paul Graham's influential writings, optimized for offline reading in EPUB and Markdown</p> <p>Featured on Hacker News front page at launch, check on GitHub for more details.</p> <p></p>"},{"location":"projects/graham/#overview","title":"Overview","text":"<p>This project automatically downloads, processes, and formats the complete collection of essays from Paul Graham's website into convenient digital formats. What started as a simple web scraping exercise turned into a substantial digital library containing over 200 essays and more than 500,000 words of influential writing on startups, technology, and entrepreneurship.</p>"},{"location":"projects/graham/#key-features","title":"Key Features","text":"<ul> <li>Complete &amp; Current: The entire collection is automatically rebuilt and   updated daily via GitHub Actions. This project was featured on the Hacker   News front page and has received over 800 GitHub stars.</li> <li>Multiple Formats: Available in both EPUB (for e-readers) and Markdown (for   developers/note-takers).</li> <li>Optimized for Reading: Clean, well-formatted text for a high-quality   reading experience on any device.</li> <li>Offline Access: Download once, read anywhere without an internet   connection.</li> <li>Open Source: Full source code is available for transparency and   community-driven development.</li> </ul>"},{"location":"projects/graham/#technical-implementation","title":"Technical Implementation","text":"<p>The project leverages the RSS feed originally created by Aaron Swartz and officially shared by Paul Graham himself, ensuring legitimate access. The automation pipeline uses several Python libraries for clean, readable output:</p> <ul> <li><code>feedparser</code> - Parses the RSS feed to retrieve essay metadata</li> <li><code>html2text</code> - Converts HTML content to a clean Markdown format</li> <li><code>htmldate</code> - Accurately extracts publication dates from web pages</li> <li><code>Unidecode</code> - Handles character encoding for consistent text formatting</li> </ul>"},{"location":"projects/graham/#getting-started","title":"Getting Started","text":""},{"location":"projects/graham/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/ofou/graham-essays.git\ncd graham-essays\nmake\n</code></pre> <p>This generates fresh EPUB and Markdown files with the latest essays.</p>"},{"location":"projects/graham/#downloads","title":"Downloads","text":"<p>Ready-to-use files (updated daily via GitHub Actions):</p> <ul> <li>\ud83c\udff7\ufe0f All Releases</li> <li>\ud83d\udcda   Direct EPUB Download</li> <li>\ud83d\udcc4   Essay Index (CSV)</li> </ul>"},{"location":"projects/graham/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Whether it's code improvements, format suggestions, or feature requests:</p> <ul> <li>Open an issue for bug reports or feature requests</li> <li>Submit a pull request for code improvements</li> <li>Share feedback on the reading experience</li> </ul> <p>The goal is to make Paul Graham's essays as accessible as possible while maintaining the highest quality reading experience.</p> <p>This is an independent project created for educational and personal use. All essay content remains the intellectual property of Paul Graham.</p>"},{"location":"projects/textube/","title":"Textube: YouTube-to-ChatGPT API","text":"<p>Built before Google integrated YouTube into Gemini natively :)</p> <p>Featured on Hacker News, currently live at textube, ChatGPT Plugin or GitHub</p>"},{"location":"projects/textube/#the-problem-i-solved","title":"The Problem I Solved","text":"<p>Before Google built YouTube integration directly into Gemini, there was no clean way to analyze YouTube videos with LLMs. You could copy-paste transcripts manually, but that was tedious and broke for long-form content. I built Textube to bridge this gap - a clean API that lets ChatGPT analyze any YouTube video or entire playlists.</p>"},{"location":"projects/textube/#technical-architecture","title":"Technical Architecture","text":"<p>FastAPI backend with two main endpoints:</p> <ul> <li><code>/watch?v={video_id}&amp;format={json|txt}</code> - Individual video transcripts   mirroring the YouTube API</li> <li><code>/playlist?list={playlist_id}</code> - Process entire playlists with metadata   mirroring the YouTube API</li> </ul> <p>Core Components:</p> <pre><code>class Captions:\n    def __init__(self, video_id: str):\n        # Uses youtube-transcript-api to fetch transcripts\n        # Calculates token counts with tiktoken for ChatGPT context limits\n        # Processes timestamps and metadata\n</code></pre> <pre><code>class Playlist:\n    def process_playlist(self) -&gt; None:\n        # Batch processes hundreds of videos\n        # Maintains video order and calculates aggregate statistics\n        # Handles playlist-level caching and integrity checks\n</code></pre> <p>Data Flow:</p> <ol> <li>Extract video/playlist IDs from YouTube URLs using regex</li> <li>Fetch transcripts via YouTube's transcript API</li> <li>Process and structure the data (timestamps, token counts, metadata)</li> <li>Cache in Google Cloud Firestore for performance</li> <li>Serve as JSON/TXT or integrate with ChatGPT plugin</li> </ol>"},{"location":"projects/textube/#key-technical-decisions","title":"Key Technical Decisions","text":"<p>Caching Strategy: Started with local JSON files, migrated to Firestore as usage scaled</p> <pre><code># Original approach - local caching\nvideo_file = os.path.join(self.base_path, self.playlist_id, f\"{video_id}.json\")\n\n# Scaled approach - Firestore with document references\nvideo_ref = db.collection(\"videos\").document(video_id)\n</code></pre> <p>Token Counting: Used <code>tiktoken</code> to calculate exact token counts for ChatGPT context windows</p> <pre><code>def _count_tokens(self, model_name: str = \"gpt-4\") -&gt; int:\n    encoding = tiktoken.encoding_for_model(model_name)\n    return len(encoding.encode(str(self)))\n</code></pre> <p>Playlist Processing: Built to handle massive playlists (Lex Fridman's 250+ videos, Dwarkesh's 72 videos = 1M+ words)</p> <ul> <li>Maintains video order through document references</li> <li>Calculates aggregate statistics (total tokens, characters, duration)</li> <li>Handles partial failures gracefully</li> </ul> <p>ChatGPT Integration: Implemented as OpenAI plugin with proper API specification</p> <ul> <li>Exposes transcript data directly to ChatGPT's context</li> <li>Handles large transcripts through chunking strategies</li> <li>Maintains conversation context across video segments</li> </ul>"},{"location":"projects/textube/#tech-stack","title":"Tech Stack","text":"<pre><code>Backend: FastAPI + Python\nDatabase: Google Cloud Firestore\nAPIs: youtube-transcript-api, pytubefix\nToken Counting: tiktoken\nFrontend: Vanilla JS + Tailwind CSS\nDeployment: Docker container\nIntegration: OpenAI Plugin API\n</code></pre>"},{"location":"projects/textube/#performance-characteristics","title":"Performance Characteristics","text":"<p>Handles edge cases:</p> <ul> <li>8-hour podcasts (Lex Fridman)</li> <li>Multi-language transcripts (English, Spanish fallbacks)</li> <li>Missing/corrupted transcript data</li> <li>Rate limiting and API failures</li> </ul> <p>Caching effectiveness:</p> <ul> <li>Instant responses for previously processed videos</li> <li>Playlist-level caching with integrity verification</li> <li>Graceful fallback to YouTube API when cache misses</li> </ul>"},{"location":"projects/textube/#open-source-proof-of-concept","title":"Open Source Proof of Concept","text":"<p>Released as open source on GitHub to demonstrate the concept and let others build upon it. The goal was to validate demand for YouTube-LLM integration and show it was technically feasible.</p> <p>Overwhelming demand: The service got so much usage that Google eventually banned it for exceeding API limits. This was actually validation - the demand was real enough that Google had to take action.</p> <p>Strategic caching: Before the ban, I managed to cache thousands of popular videos and entire playlists (Karpathy's courses, 3Blue1Brown, Lex Fridman, etc.). These cached examples still work and demonstrate the concept.</p> <p>Multiple interfaces:</p> <ul> <li>Direct API access for developers: <code>GET /watch?v={video_id}&amp;format=json</code></li> <li>ChatGPT plugin for conversational analysis: Integrated via OpenAI's plugin   system</li> <li>Simple web interface for quick access: Clean HTML form with real-time   results</li> </ul> <p>Check the API docs for more details. Here are some examples:</p> <pre><code>curl https://textube.olivares.cl/watch?v=bZQun8Y4L2A&amp;format=json | jq # State of GPT by Andrej Karpathy\n</code></pre> <p>Response:</p> <pre><code>{\n  \"character_count\": 43265,\n  \"title\": \"State of GPT | BRK216HFS\",\n  \"url\": \"https://www.youtube.com/watch?v=bZQun8Y4L2A\",\n  \"video_id\": \"bZQun8Y4L2A\",\n  \"token_count\": 9975,\n  \"transcript\": [\n    {\n      \"end\": 7.159,\n      \"start\": 0.235,\n      \"text\": \"[MUSIC]\"\n    },\n    {\n      \"end\": 8.896,\n      \"start\": 7.159,\n      \"text\": \"ANNOUNCER:\\nPlease welcome\"\n    },\n    {\n      \"end\": 10.231,\n      \"start\": 8.896,\n      \"text\": \"AI researcher and\"\n    },\n    {\n      \"end\": 21.169,\n      \"start\": 10.231,\n      \"text\": \"founding member of\\nOpenAI, Andrej Karpathy.\"\n    },\n    ...\n  ]\n}\n</code></pre> <pre><code>curl https://textube.olivares.cl/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ | jq # Neural Networks: Zero to Hero playlist\n</code></pre> <p>Response:</p> <pre><code>{\n  \"playlist_id\": \"PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\",\n  \"title\": \"Neural Networks: Zero to Hero\",\n  \"url\": \"https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\",\n  \"total_token_count\": 222771,\n  \"total_character_count\": 1066072,\n  \"video_count\": 10,\n  \"videos\": [\n    {\n      \"video_id\": \"VMj-3S1tku0\",\n      \"title\": \"The spelled-out intro to neural networks and backpropagation: building micrograd\",\n      \"token_count\": 26238,\n      \"character_count\": 123212,\n      \"local_link\": \"https://textube.olivares.cl/watch?v=VMj-3S1tku0\"\n    },\n    {\n      \"video_id\": \"PaCmpygFfXo\",\n      \"title\": \"The spelled-out intro to language modeling: building makemore\",\n      \"token_count\": 20811,\n      \"character_count\": 100179,\n      \"local_link\": \"https://textube.olivares.cl/watch?v=PaCmpygFfXo\"\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"projects/textube/#why-it-hit-hacker-news-frontpage","title":"Why It Hit Hacker News Frontpage","text":"<p>Timing: Built this before major LLM providers integrated YouTube natively. Solved a real pain point that thousands of developers/researchers had.</p> <p>Immediate utility: People could paste any YouTube URL and start analyzing the content with ChatGPT within seconds. No complex setup, no API keys.</p> <p>Scale: Handled viral traffic from HN without breaking. The Firestore caching strategy paid off when hundreds of users started processing large playlists.</p> <p>Community validation: HN commenters immediately understood the value and started sharing their own use cases - research, content analysis, learning from technical talks.</p>"},{"location":"projects/textube/#technical-impact","title":"Technical Impact","text":"<p>This project validated several architectural patterns I've used in subsequent projects:</p> <p>API-first design: Clean separation between data processing (captions.py) and API layer (app.py)</p> <p>Progressive enhancement: Started simple (single videos) then expanded to complex use cases (playlists, bulk processing)</p> <p>Caching as a first-class concern: Moved from local files to cloud database as usage patterns became clear</p> <p>LLM integration patterns: Token counting, context window management, and plugin architectures that became standard later</p>"},{"location":"projects/textube/#from-banned-to-built-in","title":"From Banned to Built-in","text":"<p>The trajectory tells the story: Google banned the service due to demand, then months later built similar functionality directly into Gemini. Classic pattern of successful proof-of-concepts getting killed then rebuilt natively.</p> <p>Open source legacy: The code remains available as a reference implementation. Shows how to:</p> <ul> <li>Build LLM integrations before official APIs exist</li> <li>Handle scale with smart caching strategies</li> <li>Design APIs that work with both humans and AI systems</li> <li>Process large-scale content for AI consumption</li> </ul> <p>Validation through prohibition: Sometimes getting banned is the strongest product-market fit signal. The demand was real enough that Google had to intervene.</p> <p>The lesson: build proof-of-concepts that push boundaries. Even if they get shut down, they demonstrate market demand and technical feasibility that larger companies eventually adopt.</p> <p>Open source on GitHub. Cached examples still work. Can be used via direct API calls or ChatGPT integration.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/films/","title":"Films","text":""},{"location":"blog/category/books/","title":"Books","text":""}]}