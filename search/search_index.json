{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello, I'm Omar","text":"<p>I'm a Software Engineer with a passion for Artificial Intelligence (AI) and Machine Learning (ML) who likes to turn complex ideas into practical applications. I previously worked as a music producer, content creator for Neura Pod, then pivoted into AI Engineering and Developer Advocate, building AI-powered products like Kilo Code coding agent, Emergent Mind RAG systems for CS research, and scalable web applications for various startups. I'm currently available for hiring, consulting, or collaboration.</p>"},{"location":"#what-i-do","title":"What I Do","text":"<ol> <li>Build production-ready AI/ML products</li> <li>Consulting and training in AI/ML for startups and enterprises</li> <li>Create technical content from internal docs to videos</li> <li>Independent research in AI, neuroscience, and machine learning</li> </ol> <p>I also enjoy making music, contributing to open-source projects, learning Mandarin (\u4f60\u597d), and exploring new places around the world.</p>"},{"location":"#lets-connect","title":"Let's Connect","text":"<p>Reach out via X, LinkedIn, or omar@olivares.cl to discuss your next project or just to chat about technology.</p>"},{"location":"blog/","title":"Blog","text":"<p>This is my blog. I write about my experiences, thoughts, and ideas.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/","title":"Jiro Dreams of Sushi","text":"<p>I just watched Jiro Dreams of Sushi, and I was really amazed by the quality of the documentary. I resonate highly with Jiro's philosophy and work ethic, and I think that his story is a great reminder that anyone can achieve greatness if they're willing to put in the time and effort. I think his insights are applicable to anyone in any field, not just sushi chefs.</p> <p>Here's a collection of quotes from a follow-up interview with Jiro and his son, Yoshikazu. Now that everyone is trying to cook at work, I think these insights are more relevant than ever.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-loving-the-work","title":"On Loving the Work","text":"<p>You must come to love your job \u2014 otherwise, it's no good.</p> <p>If you start saying things like, 'I don't like this,' or 'This isn't for me,' you'll never become a master.</p> <p>Once you've chosen your path, you move forward. Forward, forward.</p> <p>Young people today \u2014 they talk like they're already masters. But when it comes to real work, they fall short. Second-rate. Third-rate.</p> <p>That's because machines have advanced. Everyone rushes to what's easy.</p> <p>But the real work \u2014 the hand work \u2014 like sushi, or any craft done by hand\u2026 fewer and fewer people are doing it now. Especially the young.</p> <p>If they don't love it, they won't improve.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-innovation","title":"On Innovation","text":"<p>If something new tastes better \u2014 that's fine.</p> <p>But if it just looks beautiful and the flavor declines, it won't last.</p> <p>Flavor must always come first.</p> <p>He told me about octopus \u2014 tako.</p> <p>In the past, most people didn't like it.</p> <p>But I changed the way I prepared it. Improved it.</p> <p>And then, the people who used to say 'I hate tako' started asking for seconds.</p> <p>That's when you know you've got real skill.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-mastery","title":"On Mastery","text":"<p>I asked: When did you feel like a true craftsman? Like a master?</p> <p>Around 50.</p> <p>Fifty?</p> <p>Yes. Five-zero.</p> <p>Before that? You fail. Then improve. Fail again. Improve again.</p> <p>Around age 50, you start to finally create something close to what you have in mind.</p> <p>From eight years old until 50 \u2014 did he ever want to quit?</p> <p>Never. Not once.</p> <p>It was never, 'Should I quit?'</p> <p>It was always, 'How can I get better?'</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-family-and-craft","title":"On Family and Craft","text":"<p>What's it like working alongside your son all these years?</p> <p>Inside the restaurant, I don't think of him as my son.</p> <p>If I did, he wouldn't improve.</p> <p>He's a craftsman. I'm the master. Simple as that.</p> <p>His son spoke up:</p> <p>I think I'm lucky. Having a teacher like this by my side.</p> <p>But I only started feeling that way recently.</p> <p>He was strict. It was tough.</p> <p>How long has he been doing this?</p> <p>This is my 37<sup>th</sup> or 38<sup>th</sup> year.</p> <p>But I haven't even reached half his level.</p> <p>Every time I feel like I'm catching up, I look up\u2026</p> <p>\u2026and he's already a thousand steps ahead.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-the-old-days","title":"On the Old Days","text":"<p>Back then, you couldn't just say, 'I'll go here' or 'I'll quit and try that.'</p> <p>There were too many mouths to feed.</p> <p>If someone took you in, that was enough. You stayed, studied hard, and earned your place.</p> <p>Did he feel competitive with his peers?</p> <p>Not 'I won't lose' \u2014 I thought, 'I will overtake them.'</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-happiness-and-purpose","title":"On Happiness and Purpose","text":"<p>What brings more happiness \u2014 making sushi or making people happy?</p> <p>Being able to work is happiness.</p> <p>Being able to do work you love \u2014 that's a higher level of happiness.</p> <p>It's all about accumulation.</p> <p>I'm still working at 90. Work is my ikigai \u2014 my reason for living.</p> <p>He told me about a time someone asked if he'd do something fun over a three-day holiday:</p> <p>I said, 'If I don't get to work soon, I'll die of boredom.'</p> <p>I'd rather be at the restaurant.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-legacy","title":"On Legacy","text":"<p>At 60 or 70, you've probably achieved what you aimed for.</p> <p>But you don't quit.</p> <p>You think, What's next?</p> <p>That's how humans are.</p> <p>I nodded. I think I'm starting to understand.</p> <p>You're probably already thinking, 'What's next? Should I keep going?'</p> <p>But when you've accomplished something, the question becomes \u2014</p> <p>What else can I do?</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#on-struggle-and-will","title":"On Struggle and Will","text":"<p>We talked about Ren\u00e9 Redzepi, about struggle, and stubbornness.</p> <p>You have to be strong-willed and sensitive.</p> <p>That's how you get here.</p> <p>And those at the top?</p> <p>They don't retire at 70 or 80.</p> <p>They tighten their belts.</p> <p>The next accomplishment is even closer.</p> <p>This will happen to you too, I bet.</p> <p>But if I'm alive when it does, I'll say \u2014 See? I told you so.</p>"},{"location":"blog/2025/05/01/jiro-dreams-of-sushi/#one-more","title":"One More?","text":"<p>I asked if this was the end.</p> <p>He laughed.</p> <p>One more? Yes. One more.</p> <p>The not-so-secret dream: to make sushi until the Tokyo Olympics.</p> <p>He would be 95.</p> <p>During the last Olympics, I was already here.</p> <p>I want to be here for the next one, too.</p> <p>Thank you for the meal.</p>"},{"location":"blog/2024/01/01/learning-in-2025/","title":"Things I want to learn in 2025","text":"<p>This year, my focus is on exploring the intersection of neuroscience and AI. I'll be diving deep into textbooks and hands-on projects to better understand both the human brain and artificial intelligence. The following lists outline the books, textbooks, and codebases that I'm currently reading and coding from.</p>"},{"location":"blog/2024/01/01/learning-in-2025/#books","title":"Books","text":"<ul> <li> Profiles of the Future <sup>1</sup></li> <li> Artificial Intelligence: From Medieval Robots to Neural Networks <sup>2</sup></li> <li> Code: The Hidden Language of Computer Hardware and Software <sup>3</sup></li> <li> Paths, Dangers, Strategies <sup>4</sup></li> <li> How we learn: Why brains learn better than any machine... for now <sup>5</sup></li> <li> The Elements of Computing Systems: Building a Modern Computer from First Principles <sup>6</sup></li> <li> Art of Doing Science and Engineering: Learning to Learn <sup>7</sup></li> <li> But How Do It Know?: The Basic Principles of Computers for Everyone <sup>8</sup></li> <li> Life 3.0: Being Human in the Age of Artificial Intelligence <sup>9</sup></li> <li> Human Compatible: AI and the Problem of Control <sup>10</sup></li> <li> A New History of Modern Computing <sup>11</sup></li> <li> The Alignment Problem: How Can Machines Learn Human Values? <sup>12</sup></li> <li> The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn of AI <sup>13</sup></li> <li> The Singularity Is Nearer: When We Merge with AI <sup>14</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#textbooks","title":"Textbooks","text":"<ul> <li> AI Engineering: Building Applications with Foundation Models <sup>15</sup></li> <li> Artificial Intelligence: A Modern Approach <sup>16</sup></li> <li> Deep Learning: Foundations and Concepts <sup>17</sup></li> <li> The Little Book of Deep Learning <sup>18</sup></li> <li> Mathematics for Machine Learning <sup>19</sup></li> <li> Designing Machine Learning Systems <sup>20</sup></li> <li> Reinforcement Learning: An Introduction <sup>21</sup></li> <li> An Introduction to Statistical Learning: With Applications in Python <sup>22</sup></li> <li> Introduction to Information Retrieval <sup>23</sup></li> <li> Bayesian Reasoning and Machine Learning <sup>24</sup></li> <li> Neural Networks and Deep Learning <sup>25</sup></li> <li> Deep Learning <sup>26</sup></li> <li> Foundations of Machine Learning <sup>27</sup></li> <li> Grokking Machine Learning <sup>28</sup></li> <li> Deep Learning from Scratch: Building with Python from First Principles <sup>29</sup></li> <li> The Hundred-Page Machine Learning Book <sup>30</sup></li> <li> Neural Networks from Scratch in Python: Building Neural Networks in Raw Python <sup>31</sup></li> <li> Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD <sup>32</sup></li> <li> Introduction to Machine Learning <sup>33</sup></li> <li> Deep Learning with Python <sup>34</sup></li> <li> Programming Massively Parallel Processors: A Hands-on Approach <sup>35</sup></li> <li> Machine Learning with PyTorch and Scikit-Learn: Develop Machine Learning and Deep Learning Models with Python <sup>36</sup></li> <li> Probabilistic Machine Learning: An Introduction <sup>37</sup></li> <li> The Principles of Deep Learning Theory <sup>38</sup></li> <li> Understanding Deep Learning <sup>39</sup></li> <li> Dive into Deep Learning <sup>40</sup></li> <li> Multi-agent Reinforcement Learning: Foundations and Modern Approaches <sup>41</sup></li> <li> Introduction to AI Safety, Ethics and Society <sup>42</sup></li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#code","title":"Code","text":"<ul> <li> Python &amp; PyTorch by Meta, XLA by Google</li> <li> JAX by Google DeepMind -James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs. 2018. URL: http://github.com/jax-ml/jax.</li> <li> tinygrad by George Hotz &amp; tiny corp</li> <li> Hugging Face</li> <li> x-transfomers by Phil Wang (aka lucidrains)</li> <li> MLX -Awni Awni, Angelos Wathieu, Philippe Tillet, Tullie Tape, Jack Ledsam, Jacob Weber, Ari Garris, Amal Almahairi, William Gormley, and Edward Grefenstette. Mlx. 2023. URL: https://github.com/ml-explore/mlx., MLX examples &amp; axlearn by Apple</li> <li> ggml by Georgi Gerganov</li> <li> nanoGPT, micrograd, &amp; llm.c by Andrej Karpathy</li> <li> Triton by OpenAI -Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. 2019. URL: https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf.</li> <li> CUDA by Nvidia &amp; GPU Puzzles by Sasha Rush</li> </ul>"},{"location":"blog/2024/01/01/learning-in-2025/#references","title":"References","text":"<ol> <li> <p>Arthur C Clarke. Profiles of the Future. Hachette UK, 1962.\u00a0\u21a9</p> </li> <li> <p>Clifford A Pickover. Artificial Intelligence: From Medieval Robots to Neural Networks. Union Square+ ORM, 2019.\u00a0\u21a9</p> </li> <li> <p>Charles Petzold. Code: The hidden language of computer hardware and software. Microsoft Press, 2000.\u00a0\u21a9</p> </li> <li> <p>N Superintelligence Bostrom. Paths, dangers, strategies. Oxford University Press: Oxford, UK, 2014.\u00a0\u21a9</p> </li> <li> <p>Stanislas Dehaene. How we learn: Why brains learn better than any machine... for now. Penguin, 2021.\u00a0\u21a9</p> </li> <li> <p>Noam Nisan and Shimon Schocken. The elements of computing systems: building a modern computer from first principles. MIT press, 2021.\u00a0\u21a9</p> </li> <li> <p>Richard R Hamming. Art of doing science and engineering: Learning to learn. CRC Press, 1997.\u00a0\u21a9</p> </li> <li> <p>J Clark Scott. But how do it know?: the basic principles of computers for everyone. John C Scott, 2009.\u00a0\u21a9</p> </li> <li> <p>Max Tegmark. Life 3.0: Being human in the age of artificial intelligence. Vintage, 2018.\u00a0\u21a9</p> </li> <li> <p>Stuart Russell. Human compatible: AI and the problem of control. Penguin Uk, 2019.\u00a0\u21a9</p> </li> <li> <p>Thomas Haigh and Paul E Ceruzzi. A new history of modern computing. MIT Press, 2021.\u00a0\u21a9</p> </li> <li> <p>Brian Christian. The alignment problem: How can machines learn human values? Atlantic Books, 2021.\u00a0\u21a9</p> </li> <li> <p>Fei-Fei Li. The Worlds I See: Curiosity, exploration, and discovery at the dawn of AI. Flatiron books: a moment of lift book, 2023.\u00a0\u21a9</p> </li> <li> <p>Ray Kurzweil. The Singularity Is Nearer: When We Merge with AI. Random House, 2024.\u00a0\u21a9</p> </li> <li> <p>Chip Huyen. AI Engineering. O'Reilly Media, USA, 2025. ISBN 978-1801819312.\u00a0\u21a9</p> </li> <li> <p>Stuart J Russell and Peter Norvig. Artificial intelligence: a modern approach. Pearson, 2020.\u00a0\u21a9</p> </li> <li> <p>Christopher M Bishop and Hugh Bishop. Deep learning: foundations and concepts. Springer, 2024. URL: https://annas-archive.org/md\u215ad1f851e90bf7847ad9999efadc56d5b.\u00a0\u21a9</p> </li> <li> <p>Fran\u00e7ois Fleuret. The little book of deep learning. Fran\u00e7ois Fleuret, 2023.\u00a0\u21a9</p> </li> <li> <p>Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. Mathematics for machine learning. Cambridge University Press, 2020. URL: https://annas-archive.org/md5/170355f6bd92c95c84a43414cfcffec2.\u00a0\u21a9</p> </li> <li> <p>Chip Huyen. Designing machine learning systems. \" O'Reilly Media, Inc.\", 2022. URL: https://annas-archive.org/md5/f5c999ff6f4f7def83fa044b038f9390.\u00a0\u21a9</p> </li> <li> <p>Barto Andrew and Sutton Richard S. Reinforcement Learning: An Introduction. The MIT Press, 2018. URL: https://annas-archive.org/md5/cd556761e796b7c76c1c7a570482eba7.\u00a0\u21a9</p> </li> <li> <p>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. An introduction to statistical learning: With applications in Python. Springer Nature, 2023. URL: https://www.statlearning.com.\u00a0\u21a9</p> </li> <li> <p>Hinrich Sch\u00fctze, Christopher D Manning, and Prabhakar Raghavan. Introduction to information retrieval. Volume 39. Cambridge University Press Cambridge, 2008. URL: https://nlp.stanford.edu/IR-book/information-retrieval-book.html.\u00a0\u21a9</p> </li> <li> <p>David Barber. Bayesian reasoning and machine learning. Cambridge University Press, 2012. URL: http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/020217.pdf.\u00a0\u21a9</p> </li> <li> <p>A Nielsen. Neural networks and deep learning. 2015. URL: https://lit2talks.com/upload/neuralnetworksanddeeplearning.pdf.\u00a0\u21a9</p> </li> <li> <p>Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. URL: https://annas-archive.org/md5/147862ee2c14791ac71520dca611a7d5.\u00a0\u21a9</p> </li> <li> <p>Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2018. URL: https://annas-archive.org/md5/1495446fc912817f0cd2986905eb8389.\u00a0\u21a9</p> </li> <li> <p>Luis Serrano. Grokking machine learning. Simon and Schuster, 2021.\u00a0\u21a9</p> </li> <li> <p>Seth Weidman. Deep learning from scratch: Building with python from first principles. O'Reilly Media, 2019. URL: https://annas-archive.org/md5/af1400a89b34bf0ac0a4a35d108d67ad.\u00a0\u21a9</p> </li> <li> <p>Andriy Burkov. The hundred-page machine learning book. Andriy Burkov Quebec City, QC, Canada, 2019. URL: https://annas-archive.org/md5/07912d7f54c6ef124c53d18cc69859bf.\u00a0\u21a9</p> </li> <li> <p>Harrison Kinsley and Daniel Kukie\u0142a. Neural Networks from Scratch in Python: Building Neural Networks in Raw Python. Verlag nicht ermittelbar, 2020.\u00a0\u21a9</p> </li> <li> <p>Jeremy Howard and Sylvain Gugger. Deep Learning for Coders with fastai and PyTorch. O'Reilly Media, 2020. URL: https://annas-archive.org/md5/7e55aada5f4f30daa2a340c5409c896c.\u00a0\u21a9</p> </li> <li> <p>Ethem Alpaydin. Introduction to machine learning. MIT press, 2020. URL: https://annas-archive.org/md5/4e078be579f512b629817cbc83b62e93.\u00a0\u21a9</p> </li> <li> <p>Francois Chollet. Deep learning with Python. Simon and Schuster, 2021. URL: https://annas-archive.org/md5/59737ebea569c42afd3c3acabf2a5f9a.\u00a0\u21a9</p> </li> <li> <p>David B Kirk and Wen-mei W Hwu. Programming massively parallel processors: a hands-on approach. Morgan Kaufmann, 2016.\u00a0\u21a9</p> </li> <li> <p>Sebastian Raschka, Yuxi Hayden Liu, Vahid Mirjalili, and Dmytro Dzhulgakov. Machine Learning with PyTorch and Scikit-Learn: Develop machine learning and deep learning models with Python. Packt Publishing Ltd, 2022. URL: https://annas-archive.org/md5/c1ae0c9c5eaff8b929972790879197e6.\u00a0\u21a9</p> </li> <li> <p>Kevin P Murphy. Probabilistic machine learning: an introduction. MIT press, 2022. URL: https://annas-archive.org/md5/9a26e8cc5fa4c0b40d7a91443f9b71f8.\u00a0\u21a9</p> </li> <li> <p>Daniel A Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory. Cambridge University Press Cambridge, MA, USA, 2022. URL: https://annas-archive.org/md5/e1fdfd303f45ed2689fe3c0dcfe1db62.\u00a0\u21a9</p> </li> <li> <p>Simon J.D. Prince. Understanding Deep Learning. The MIT Press, 2023. URL: http://udlbook.com.\u00a0\u21a9</p> </li> <li> <p>Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. \\url https://D2L.ai. URL: https://d2l.ai.\u00a0\u21a9</p> </li> <li> <p>Stefano V Albrecht, Filippos Christianos, and Lukas Sch\u00e4fer. Multi-agent reinforcement learning: Foundations and modern approaches. MIT Press, 2024.\u00a0\u21a9</p> </li> <li> <p>Dan Hendrycks. Introduction to AI safety, ethics and society. Dan Hendrycks, 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"projects/graham/","title":"Graham Essays","text":"<p>A comprehensive digital archive of Paul Graham's influential writings, optimized for offline reading in EPUB and Markdown</p> <p>Featured on Hacker News front page at launch, check on GitHub for more details.</p> <p></p>"},{"location":"projects/graham/#overview","title":"Overview","text":"<p>This project automatically downloads, processes, and formats the complete collection of essays from Paul Graham's website into convenient digital formats. What started as a simple web scraping exercise turned into a substantial digital library containing over 200 essays and more than 500,000 words of influential writing on startups, technology, and entrepreneurship.</p>"},{"location":"projects/graham/#key-features","title":"Key Features","text":"<ul> <li>Complete &amp; Current: The entire collection is automatically rebuilt and   updated daily via GitHub Actions. This project was featured on the Hacker   News front page and has received over 800 GitHub stars.</li> <li>Multiple Formats: Available in both EPUB (for e-readers) and Markdown (for   developers/note-takers).</li> <li>Optimized for Reading: Clean, well-formatted text for a high-quality   reading experience on any device.</li> <li>Offline Access: Download once, read anywhere without an internet   connection.</li> <li>Open Source: Full source code is available for transparency and   community-driven development.</li> </ul>"},{"location":"projects/graham/#technical-implementation","title":"Technical Implementation","text":"<p>The project leverages the RSS feed originally created by Aaron Swartz and officially shared by Paul Graham himself, ensuring legitimate access. The automation pipeline uses several Python libraries for clean, readable output:</p> <ul> <li><code>feedparser</code> - Parses the RSS feed to retrieve essay metadata</li> <li><code>html2text</code> - Converts HTML content to a clean Markdown format</li> <li><code>htmldate</code> - Accurately extracts publication dates from web pages</li> <li><code>Unidecode</code> - Handles character encoding for consistent text formatting</li> </ul>"},{"location":"projects/graham/#getting-started","title":"Getting Started","text":""},{"location":"projects/graham/#quick-start","title":"Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/ofou/graham-essays.git\ncd graham-essays\nmake\n</code></pre> <p>This generates fresh EPUB and Markdown files with the latest essays.</p>"},{"location":"projects/graham/#downloads","title":"Downloads","text":"<p>Ready-to-use files (updated daily via GitHub Actions):</p> <ul> <li>\ud83c\udff7\ufe0f All Releases</li> <li>\ud83d\udcda   Direct EPUB Download</li> <li>\ud83d\udcc4   Essay Index (CSV)</li> </ul>"},{"location":"projects/graham/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Whether it's code improvements, format suggestions, or feature requests:</p> <ul> <li>Open an issue for bug reports or feature requests</li> <li>Submit a pull request for code improvements</li> <li>Share feedback on the reading experience</li> </ul> <p>The goal is to make Paul Graham's essays as accessible as possible while maintaining the highest quality reading experience.</p> <p>This is an independent project created for educational and personal use. All essay content remains the intellectual property of Paul Graham.</p>"},{"location":"projects/textube/","title":"Textube: YouTube-to-ChatGPT API","text":"<p>Built before Google integrated YouTube into Gemini natively :)</p> <p>Featured on Hacker News, currently live at textube, ChatGPT Plugin or GitHub</p>"},{"location":"projects/textube/#the-problem-i-solved","title":"The Problem I Solved","text":"<p>Before Google built YouTube integration directly into Gemini, there was no clean way to analyze YouTube videos with LLMs. You could copy-paste transcripts manually, but that was tedious and broke for long-form content. I built Textube to bridge this gap - a clean API that lets ChatGPT analyze any YouTube video or entire playlists.</p>"},{"location":"projects/textube/#technical-architecture","title":"Technical Architecture","text":"<p>FastAPI backend with two main endpoints:</p> <ul> <li><code>/watch?v={video_id}&amp;format={json|txt}</code> - Individual video transcripts   mirroring the YouTube API</li> <li><code>/playlist?list={playlist_id}</code> - Process entire playlists with metadata   mirroring the YouTube API</li> </ul> <p>Core Components:</p> <pre><code>class Captions:\n    def __init__(self, video_id: str):\n        # Uses youtube-transcript-api to fetch transcripts\n        # Calculates token counts with tiktoken for ChatGPT context limits\n        # Processes timestamps and metadata\n</code></pre> <pre><code>class Playlist:\n    def process_playlist(self) -&gt; None:\n        # Batch processes hundreds of videos\n        # Maintains video order and calculates aggregate statistics\n        # Handles playlist-level caching and integrity checks\n</code></pre> <p>Data Flow:</p> <ol> <li>Extract video/playlist IDs from YouTube URLs using regex</li> <li>Fetch transcripts via YouTube's transcript API</li> <li>Process and structure the data (timestamps, token counts, metadata)</li> <li>Cache in Google Cloud Firestore for performance</li> <li>Serve as JSON/TXT or integrate with ChatGPT plugin</li> </ol>"},{"location":"projects/textube/#key-technical-decisions","title":"Key Technical Decisions","text":"<p>Caching Strategy: Started with local JSON files, migrated to Firestore as usage scaled</p> <pre><code># Original approach - local caching\nvideo_file = os.path.join(self.base_path, self.playlist_id, f\"{video_id}.json\")\n\n# Scaled approach - Firestore with document references\nvideo_ref = db.collection(\"videos\").document(video_id)\n</code></pre> <p>Token Counting: Used <code>tiktoken</code> to calculate exact token counts for ChatGPT context windows</p> <pre><code>def _count_tokens(self, model_name: str = \"gpt-4\") -&gt; int:\n    encoding = tiktoken.encoding_for_model(model_name)\n    return len(encoding.encode(str(self)))\n</code></pre> <p>Playlist Processing: Built to handle massive playlists (Lex Fridman's 250+ videos, Dwarkesh's 72 videos = 1M+ words)</p> <ul> <li>Maintains video order through document references</li> <li>Calculates aggregate statistics (total tokens, characters, duration)</li> <li>Handles partial failures gracefully</li> </ul> <p>ChatGPT Integration: Implemented as OpenAI plugin with proper API specification</p> <ul> <li>Exposes transcript data directly to ChatGPT's context</li> <li>Handles large transcripts through chunking strategies</li> <li>Maintains conversation context across video segments</li> </ul>"},{"location":"projects/textube/#tech-stack","title":"Tech Stack","text":"<pre><code>Backend: FastAPI + Python\nDatabase: Google Cloud Firestore\nAPIs: youtube-transcript-api, pytubefix\nToken Counting: tiktoken\nFrontend: Vanilla JS + Tailwind CSS\nDeployment: Docker container\nIntegration: OpenAI Plugin API\n</code></pre>"},{"location":"projects/textube/#performance-characteristics","title":"Performance Characteristics","text":"<p>Handles edge cases:</p> <ul> <li>8-hour podcasts (Lex Fridman)</li> <li>Multi-language transcripts (English, Spanish fallbacks)</li> <li>Missing/corrupted transcript data</li> <li>Rate limiting and API failures</li> </ul> <p>Caching effectiveness:</p> <ul> <li>Instant responses for previously processed videos</li> <li>Playlist-level caching with integrity verification</li> <li>Graceful fallback to YouTube API when cache misses</li> </ul>"},{"location":"projects/textube/#open-source-proof-of-concept","title":"Open Source Proof of Concept","text":"<p>Released as open source on GitHub to demonstrate the concept and let others build upon it. The goal was to validate demand for YouTube-LLM integration and show it was technically feasible.</p> <p>Overwhelming demand: The service got so much usage that Google eventually banned it for exceeding API limits. This was actually validation - the demand was real enough that Google had to take action.</p> <p>Strategic caching: Before the ban, I managed to cache thousands of popular videos and entire playlists (Karpathy's courses, 3Blue1Brown, Lex Fridman, etc.). These cached examples still work and demonstrate the concept.</p> <p>Multiple interfaces:</p> <ul> <li>Direct API access for developers: <code>GET /watch?v={video_id}&amp;format=json</code></li> <li>ChatGPT plugin for conversational analysis: Integrated via OpenAI's plugin   system</li> <li>Simple web interface for quick access: Clean HTML form with real-time   results</li> </ul> <p>Check the API docs for more details. Here are some examples:</p> <pre><code>curl https://textube.olivares.cl/watch?v=bZQun8Y4L2A&amp;format=json | jq # State of GPT by Andrej Karpathy\n</code></pre> <p>Response:</p> <pre><code>{\n  \"character_count\": 43265,\n  \"title\": \"State of GPT | BRK216HFS\",\n  \"url\": \"https://www.youtube.com/watch?v=bZQun8Y4L2A\",\n  \"video_id\": \"bZQun8Y4L2A\",\n  \"token_count\": 9975,\n  \"transcript\": [\n    {\n      \"end\": 7.159,\n      \"start\": 0.235,\n      \"text\": \"[MUSIC]\"\n    },\n    {\n      \"end\": 8.896,\n      \"start\": 7.159,\n      \"text\": \"ANNOUNCER:\\nPlease welcome\"\n    },\n    {\n      \"end\": 10.231,\n      \"start\": 8.896,\n      \"text\": \"AI researcher and\"\n    },\n    {\n      \"end\": 21.169,\n      \"start\": 10.231,\n      \"text\": \"founding member of\\nOpenAI, Andrej Karpathy.\"\n    },\n    ...\n  ]\n}\n</code></pre> <pre><code>curl https://textube.olivares.cl/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ | jq # Neural Networks: Zero to Hero playlist\n</code></pre> <p>Response:</p> <pre><code>{\n  \"playlist_id\": \"PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\",\n  \"title\": \"Neural Networks: Zero to Hero\",\n  \"url\": \"https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ\",\n  \"total_token_count\": 222771,\n  \"total_character_count\": 1066072,\n  \"video_count\": 10,\n  \"videos\": [\n    {\n      \"video_id\": \"VMj-3S1tku0\",\n      \"title\": \"The spelled-out intro to neural networks and backpropagation: building micrograd\",\n      \"token_count\": 26238,\n      \"character_count\": 123212,\n      \"local_link\": \"https://textube.olivares.cl/watch?v=VMj-3S1tku0\"\n    },\n    {\n      \"video_id\": \"PaCmpygFfXo\",\n      \"title\": \"The spelled-out intro to language modeling: building makemore\",\n      \"token_count\": 20811,\n      \"character_count\": 100179,\n      \"local_link\": \"https://textube.olivares.cl/watch?v=PaCmpygFfXo\"\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"projects/textube/#why-it-hit-hacker-news-frontpage","title":"Why It Hit Hacker News Frontpage","text":"<p>Timing: Built this before major LLM providers integrated YouTube natively. Solved a real pain point that thousands of developers/researchers had.</p> <p>Immediate utility: People could paste any YouTube URL and start analyzing the content with ChatGPT within seconds. No complex setup, no API keys.</p> <p>Scale: Handled viral traffic from HN without breaking. The Firestore caching strategy paid off when hundreds of users started processing large playlists.</p> <p>Community validation: HN commenters immediately understood the value and started sharing their own use cases - research, content analysis, learning from technical talks.</p>"},{"location":"projects/textube/#technical-impact","title":"Technical Impact","text":"<p>This project validated several architectural patterns I've used in subsequent projects:</p> <p>API-first design: Clean separation between data processing (captions.py) and API layer (app.py)</p> <p>Progressive enhancement: Started simple (single videos) then expanded to complex use cases (playlists, bulk processing)</p> <p>Caching as a first-class concern: Moved from local files to cloud database as usage patterns became clear</p> <p>LLM integration patterns: Token counting, context window management, and plugin architectures that became standard later</p>"},{"location":"projects/textube/#from-banned-to-built-in","title":"From Banned to Built-in","text":"<p>The trajectory tells the story: Google banned the service due to demand, then months later built similar functionality directly into Gemini. Classic pattern of successful proof-of-concepts getting killed then rebuilt natively.</p> <p>Open source legacy: The code remains available as a reference implementation. Shows how to:</p> <ul> <li>Build LLM integrations before official APIs exist</li> <li>Handle scale with smart caching strategies</li> <li>Design APIs that work with both humans and AI systems</li> <li>Process large-scale content for AI consumption</li> </ul> <p>Validation through prohibition: Sometimes getting banned is the strongest product-market fit signal. The demand was real enough that Google had to intervene.</p> <p>The lesson: build proof-of-concepts that push boundaries. Even if they get shut down, they demonstrate market demand and technical feasibility that larger companies eventually adopt.</p> <p>Open source on GitHub. Cached examples still work. Can be used via direct API calls or ChatGPT integration.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/films/","title":"Films","text":""},{"location":"blog/category/general/","title":"General","text":""}]}